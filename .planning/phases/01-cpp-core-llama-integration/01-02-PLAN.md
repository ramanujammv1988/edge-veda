---
phase: 01-cpp-core-llama-integration
plan: 02
type: execute
wave: 2
depends_on: ["01-01"]
files_modified:
  - core/src/engine.cpp
  - core/include/edge_veda.h
autonomous: true

must_haves:
  truths:
    - "ev_init loads a GGUF model file using llama.cpp"
    - "ev_generate produces text output from a prompt"
    - "ev_free releases all llama.cpp resources without leaks"
    - "ev_get_memory_usage returns actual memory consumption"
  artifacts:
    - path: "core/src/engine.cpp"
      provides: "Working inference engine"
      contains: "llama_load_model_from_file"
      min_lines: 400
    - path: "core/include/edge_veda.h"
      provides: "C API header"
  key_links:
    - from: "core/src/engine.cpp"
      to: "llama.cpp"
      via: "llama_* function calls"
      pattern: "llama_(init|load|decode|sample|free)"
    - from: "core/src/engine.cpp"
      to: "memory_guard.cpp"
      via: "memory_guard_* calls"
      pattern: "memory_guard_"
---

<objective>
Implement the C++ inference engine by replacing stub code with real llama.cpp API calls.

Purpose: This is the core inference logic. Without a working engine, no text generation is possible. The engine must properly initialize llama.cpp, load models, run inference, and clean up resources.

Output: Fully functional engine.cpp that can load GGUF models and generate text using llama.cpp.
</objective>

<execution_context>
@/Users/ram/.claude/get-shit-done/workflows/execute-plan.md
@/Users/ram/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/phases/01-cpp-core-llama-integration/01-01-SUMMARY.md
@core/src/engine.cpp
@core/src/memory_guard.cpp
@core/include/edge_veda.h
</context>

<tasks>

<task type="auto">
  <name>Task 1: Implement ev_init with llama.cpp model loading</name>
  <files>
    core/src/engine.cpp
  </files>
  <action>
Replace the stub ev_init implementation with real llama.cpp model loading.

**Required includes** (at top of file):
```cpp
#ifdef EDGE_VEDA_LLAMA_ENABLED
#include "llama.h"
#include "ggml.h"
#endif
```

**Update ev_context_impl struct** to hold llama.cpp handles:
```cpp
#ifdef EDGE_VEDA_LLAMA_ENABLED
    llama_model* model = nullptr;
    llama_context* llama_ctx = nullptr;
    llama_sampler* sampler = nullptr;
#endif
```

**Implement ev_init:**
```cpp
ev_context ev_init(const ev_config* config, ev_error_t* error) {
    ev_error_t err = EV_SUCCESS;

    if (!config || !config->model_path) {
        if (error) *error = EV_ERROR_INVALID_PARAM;
        return nullptr;
    }

    // Allocate context
    ev_context ctx = new (std::nothrow) ev_context_impl();
    if (!ctx) {
        if (error) *error = EV_ERROR_OUT_OF_MEMORY;
        return nullptr;
    }

    ctx->config = *config;
    ctx->model_path = config->model_path;
    ctx->memory_limit = config->memory_limit_bytes;
    ctx->auto_unload = config->auto_unload_on_memory_pressure;

    // Detect backend
    ctx->active_backend = (config->backend == EV_BACKEND_AUTO)
        ? ev_detect_backend()
        : config->backend;

    if (!ev_is_backend_available(ctx->active_backend)) {
        ctx->last_error = "Backend not available";
        delete ctx;
        if (error) *error = EV_ERROR_UNSUPPORTED_BACKEND;
        return nullptr;
    }

#ifdef EDGE_VEDA_LLAMA_ENABLED
    // Initialize llama.cpp backend
    llama_backend_init();

    // Configure model parameters
    llama_model_params model_params = llama_model_default_params();
    model_params.n_gpu_layers = config->gpu_layers;
    model_params.use_mmap = config->use_mmap;
    model_params.use_mlock = config->use_mlock;

    // Load model
    ctx->model = llama_load_model_from_file(ctx->model_path.c_str(), model_params);
    if (!ctx->model) {
        ctx->last_error = "Failed to load model from: " + ctx->model_path;
        llama_backend_free();
        delete ctx;
        if (error) *error = EV_ERROR_MODEL_LOAD_FAILED;
        return nullptr;
    }

    // Configure context parameters
    llama_context_params ctx_params = llama_context_default_params();
    ctx_params.n_ctx = config->context_size > 0 ? config->context_size : 2048;
    ctx_params.n_batch = config->batch_size > 0 ? config->batch_size : 512;
    ctx_params.n_threads = config->num_threads > 0 ? config->num_threads : 4;
    ctx_params.n_threads_batch = ctx_params.n_threads;

    // Create llama context
    ctx->llama_ctx = llama_new_context_with_model(ctx->model, ctx_params);
    if (!ctx->llama_ctx) {
        ctx->last_error = "Failed to create llama context";
        llama_free_model(ctx->model);
        llama_backend_free();
        delete ctx;
        if (error) *error = EV_ERROR_BACKEND_INIT_FAILED;
        return nullptr;
    }

    ctx->model_loaded = true;

    // Set up memory monitoring
    if (ctx->memory_limit > 0) {
        memory_guard_set_limit(ctx->memory_limit);
    }

    if (error) *error = EV_SUCCESS;
    return ctx;
#else
    ctx->last_error = "llama.cpp not compiled - library built without LLM support";
    delete ctx;
    if (error) *error = EV_ERROR_NOT_IMPLEMENTED;
    return nullptr;
#endif
}
```

**Implement ev_free:**
```cpp
void ev_free(ev_context ctx) {
    if (!ctx) return;

    std::lock_guard<std::mutex> lock(ctx->mutex);

#ifdef EDGE_VEDA_LLAMA_ENABLED
    if (ctx->sampler) {
        llama_sampler_free(ctx->sampler);
        ctx->sampler = nullptr;
    }
    if (ctx->llama_ctx) {
        llama_free(ctx->llama_ctx);
        ctx->llama_ctx = nullptr;
    }
    if (ctx->model) {
        llama_free_model(ctx->model);
        ctx->model = nullptr;
    }
    llama_backend_free();
#endif

    delete ctx;
}
```

**Why this approach:**
- llama_backend_init/free manages global state (Metal shaders, etc.)
- Model params control GPU offloading via n_gpu_layers
- Context params control memory allocation via n_ctx
- Proper cleanup order: sampler -> context -> model -> backend
  </action>
  <verify>
Code compiles (will be verified in build plan):
```bash
# Check that key llama.cpp calls are present
grep -c "llama_load_model_from_file" core/src/engine.cpp  # >= 1
grep -c "llama_new_context_with_model" core/src/engine.cpp  # >= 1
grep -c "llama_free" core/src/engine.cpp  # >= 2
```
  </verify>
  <done>
- ev_init calls llama_load_model_from_file
- ev_init calls llama_new_context_with_model
- ev_free properly releases all resources
- Error handling for each failure point
  </done>
</task>

<task type="auto">
  <name>Task 2: Implement ev_generate with llama.cpp inference</name>
  <files>
    core/src/engine.cpp
  </files>
  <action>
Implement the core text generation logic using llama.cpp.

**Add helper function for tokenization:**
```cpp
#ifdef EDGE_VEDA_LLAMA_ENABLED
static std::vector<llama_token> tokenize_prompt(
    const llama_model* model,
    const std::string& text,
    bool add_bos
) {
    // Get max tokens needed
    int n_tokens = text.length() + (add_bos ? 1 : 0);
    std::vector<llama_token> tokens(n_tokens);

    // Tokenize
    n_tokens = llama_tokenize(model, text.c_str(), text.length(),
                              tokens.data(), tokens.size(), add_bos, false);

    if (n_tokens < 0) {
        tokens.resize(-n_tokens);
        n_tokens = llama_tokenize(model, text.c_str(), text.length(),
                                  tokens.data(), tokens.size(), add_bos, false);
    }

    tokens.resize(n_tokens);
    return tokens;
}
#endif
```

**Add helper to create sampler:**
```cpp
#ifdef EDGE_VEDA_LLAMA_ENABLED
static llama_sampler* create_sampler(const ev_generation_params& params) {
    llama_sampler_chain_params chain_params = llama_sampler_chain_default_params();
    llama_sampler* sampler = llama_sampler_chain_init(chain_params);

    // Add samplers in order: penalties -> top-k -> top-p -> temperature -> dist
    llama_sampler_chain_add(sampler,
        llama_sampler_init_penalties(
            LLAMA_DEFAULT_SEED,
            LLAMA_TOKEN_NULL,  // No special tokens
            LLAMA_TOKEN_NULL,
            64,                // penalty_last_n
            params.repeat_penalty,
            params.frequency_penalty,
            params.presence_penalty,
            false,             // penalize_nl
            false              // ignore_eos
        ));

    if (params.top_k > 0) {
        llama_sampler_chain_add(sampler, llama_sampler_init_top_k(params.top_k));
    }

    if (params.top_p < 1.0f) {
        llama_sampler_chain_add(sampler, llama_sampler_init_top_p(params.top_p, 1));
    }

    if (params.temperature > 0.0f) {
        llama_sampler_chain_add(sampler, llama_sampler_init_temp(params.temperature));
    }

    llama_sampler_chain_add(sampler, llama_sampler_init_dist(LLAMA_DEFAULT_SEED));

    return sampler;
}
#endif
```

**Implement ev_generate:**
```cpp
ev_error_t ev_generate(
    ev_context ctx,
    const char* prompt,
    const ev_generation_params* params,
    char** output
) {
    if (!ctx || !prompt || !output) {
        return EV_ERROR_INVALID_PARAM;
    }

    if (!ev_is_valid(ctx)) {
        return EV_ERROR_CONTEXT_INVALID;
    }

    std::lock_guard<std::mutex> lock(ctx->mutex);

    ev_generation_params gen_params;
    if (params) {
        gen_params = *params;
    } else {
        ev_generation_params_default(&gen_params);
    }

#ifdef EDGE_VEDA_LLAMA_ENABLED
    // Clear KV cache for fresh generation
    llama_kv_cache_clear(ctx->llama_ctx);

    // Tokenize prompt
    std::vector<llama_token> tokens = tokenize_prompt(ctx->model, prompt, true);
    if (tokens.empty()) {
        ctx->last_error = "Failed to tokenize prompt";
        return EV_ERROR_INFERENCE_FAILED;
    }

    // Check context size
    int n_ctx = llama_n_ctx(ctx->llama_ctx);
    if ((int)tokens.size() > n_ctx - 4) {
        ctx->last_error = "Prompt too long for context size";
        return EV_ERROR_INFERENCE_FAILED;
    }

    // Create batch for prompt processing
    llama_batch batch = llama_batch_get_one(tokens.data(), tokens.size());

    // Evaluate prompt
    if (llama_decode(ctx->llama_ctx, batch) != 0) {
        ctx->last_error = "Failed to evaluate prompt";
        return EV_ERROR_INFERENCE_FAILED;
    }

    // Create sampler
    llama_sampler* sampler = create_sampler(gen_params);
    if (!sampler) {
        ctx->last_error = "Failed to create sampler";
        return EV_ERROR_INFERENCE_FAILED;
    }

    // Generate tokens
    std::string result;
    int n_cur = tokens.size();
    llama_token eos_token = llama_token_eos(ctx->model);

    for (int i = 0; i < gen_params.max_tokens; ++i) {
        // Sample next token
        llama_token new_token = llama_sampler_sample(sampler, ctx->llama_ctx, -1);

        // Check for EOS
        if (llama_token_is_eog(ctx->model, new_token)) {
            break;
        }

        // Convert token to text
        char buf[256];
        int n = llama_token_to_piece(ctx->model, new_token, buf, sizeof(buf), 0, true);
        if (n > 0) {
            result.append(buf, n);
        }

        // Prepare next batch
        batch = llama_batch_get_one(&new_token, 1);

        // Evaluate
        if (llama_decode(ctx->llama_ctx, batch) != 0) {
            llama_sampler_free(sampler);
            ctx->last_error = "Failed during generation";
            return EV_ERROR_INFERENCE_FAILED;
        }

        n_cur++;
    }

    llama_sampler_free(sampler);

    // Allocate output string
    *output = static_cast<char*>(std::malloc(result.size() + 1));
    if (!*output) {
        return EV_ERROR_OUT_OF_MEMORY;
    }
    std::memcpy(*output, result.c_str(), result.size() + 1);

    return EV_SUCCESS;
#else
    ctx->last_error = "llama.cpp not compiled";
    return EV_ERROR_NOT_IMPLEMENTED;
#endif
}
```

**Why this implementation:**
- llama_kv_cache_clear resets context for each generation (stateless API)
- llama_batch_get_one is simpler than manual batch management
- Sampler chain follows recommended order: penalties -> top_k -> top_p -> temp -> dist
- EOS check uses llama_token_is_eog (handles both EOS and EOT tokens)
- Token-to-text uses llama_token_to_piece with special token handling
  </action>
  <verify>
```bash
# Check generation logic is present
grep -c "llama_decode" core/src/engine.cpp  # >= 2
grep -c "llama_sampler_sample" core/src/engine.cpp  # >= 1
grep -c "llama_token_to_piece" core/src/engine.cpp  # >= 1
```
  </verify>
  <done>
- ev_generate tokenizes prompt
- ev_generate runs inference loop
- ev_generate samples tokens using sampler chain
- ev_generate converts tokens to text
- ev_generate handles EOS properly
  </done>
</task>

<task type="auto">
  <name>Task 3: Implement memory and model info functions</name>
  <files>
    core/src/engine.cpp
  </files>
  <action>
Complete the remaining API functions that expose llama.cpp state.

**Implement ev_get_memory_usage:**
```cpp
ev_error_t ev_get_memory_usage(ev_context ctx, ev_memory_stats* stats) {
    if (!ctx || !stats) {
        return EV_ERROR_INVALID_PARAM;
    }

    std::lock_guard<std::mutex> lock(ctx->mutex);
    std::memset(stats, 0, sizeof(ev_memory_stats));

    // Get platform memory from memory guard
    stats->current_bytes = memory_guard_get_current_usage();
    stats->peak_bytes = ctx->peak_memory_bytes;
    stats->limit_bytes = ctx->memory_limit;

#ifdef EDGE_VEDA_LLAMA_ENABLED
    if (ctx->model) {
        // Get model size (approximate)
        stats->model_bytes = llama_model_size(ctx->model);
    }
    if (ctx->llama_ctx) {
        // Context memory usage
        stats->context_bytes = llama_state_get_size(ctx->llama_ctx);
    }
#endif

    // Update peak
    if (stats->current_bytes > ctx->peak_memory_bytes) {
        ctx->peak_memory_bytes = stats->current_bytes;
    }

    return EV_SUCCESS;
}
```

**Implement ev_get_model_info:**
```cpp
ev_error_t ev_get_model_info(ev_context ctx, ev_model_info* info) {
    if (!ctx || !info) {
        return EV_ERROR_INVALID_PARAM;
    }

    if (!ev_is_valid(ctx)) {
        return EV_ERROR_CONTEXT_INVALID;
    }

    std::lock_guard<std::mutex> lock(ctx->mutex);
    std::memset(info, 0, sizeof(ev_model_info));

#ifdef EDGE_VEDA_LLAMA_ENABLED
    // Model description
    static char model_desc[256];
    llama_model_desc(ctx->model, model_desc, sizeof(model_desc));
    info->name = model_desc;

    // Architecture (extract from description)
    info->architecture = "llama";  // Most GGUF models are llama-based

    // Parameters
    info->num_parameters = llama_model_n_params(ctx->model);

    // Context and embedding info
    info->context_length = llama_n_ctx(ctx->llama_ctx);
    info->embedding_dim = llama_n_embd(ctx->model);
    info->num_layers = llama_n_layer(ctx->model);

    return EV_SUCCESS;
#else
    return EV_ERROR_NOT_IMPLEMENTED;
#endif
}
```

**Implement ev_reset:**
```cpp
ev_error_t ev_reset(ev_context ctx) {
    if (!ctx) {
        return EV_ERROR_INVALID_PARAM;
    }

    if (!ev_is_valid(ctx)) {
        return EV_ERROR_CONTEXT_INVALID;
    }

    std::lock_guard<std::mutex> lock(ctx->mutex);

#ifdef EDGE_VEDA_LLAMA_ENABLED
    llama_kv_cache_clear(ctx->llama_ctx);
    return EV_SUCCESS;
#else
    return EV_ERROR_NOT_IMPLEMENTED;
#endif
}
```

**Implement ev_memory_cleanup:**
```cpp
ev_error_t ev_memory_cleanup(ev_context ctx) {
    if (!ctx) {
        return EV_ERROR_INVALID_PARAM;
    }

    std::lock_guard<std::mutex> lock(ctx->mutex);

#ifdef EDGE_VEDA_LLAMA_ENABLED
    // Clear KV cache to free memory
    if (ctx->llama_ctx) {
        llama_kv_cache_clear(ctx->llama_ctx);
    }
#endif

    // Trigger platform memory cleanup
    memory_guard_cleanup();

    return EV_SUCCESS;
}
```

**Update ev_set_verbose:**
```cpp
static bool g_verbose = false;

void ev_set_verbose(bool enable) {
    g_verbose = enable;

#ifdef EDGE_VEDA_LLAMA_ENABLED
    // llama.cpp uses log callback, set it based on verbosity
    if (enable) {
        llama_log_set([](enum ggml_log_level level, const char* text, void* user_data) {
            fprintf(stderr, "[llama] %s", text);
        }, nullptr);
    } else {
        llama_log_set([](enum ggml_log_level level, const char* text, void* user_data) {
            // Suppress all but errors
            if (level == GGML_LOG_LEVEL_ERROR) {
                fprintf(stderr, "[llama] %s", text);
            }
        }, nullptr);
    }
#endif
}
```
  </action>
  <verify>
```bash
# Verify all API functions are implemented
grep -c "ev_get_memory_usage" core/src/engine.cpp
grep -c "ev_get_model_info" core/src/engine.cpp
grep -c "ev_reset" core/src/engine.cpp
grep -c "llama_model_size" core/src/engine.cpp
```
  </verify>
  <done>
- ev_get_memory_usage returns model and context memory
- ev_get_model_info returns model metadata
- ev_reset clears KV cache
- ev_memory_cleanup frees memory
- ev_set_verbose controls logging
  </done>
</task>

</tasks>

<verification>
After all tasks complete:

1. **All stub implementations replaced:**
   ```bash
   grep -c "TODO:" core/src/engine.cpp  # Should be 0 or minimal
   grep -c "NOT_IMPLEMENTED" core/src/engine.cpp  # Only in #else blocks
   ```

2. **All llama.cpp functions present:**
   ```bash
   grep -E "llama_(init|load|decode|sample|free)" core/src/engine.cpp | wc -l
   # Should be > 5
   ```

3. **Error handling complete:**
   ```bash
   grep -c "EV_ERROR_" core/src/engine.cpp  # Should be > 15
   ```

4. **Memory guard integration:**
   ```bash
   grep -c "memory_guard_" core/src/engine.cpp  # Should be >= 3
   ```
</verification>

<success_criteria>
1. ev_init loads models using llama_load_model_from_file
2. ev_generate produces text using llama_decode + llama_sampler_sample
3. ev_free releases all resources without leaks
4. ev_get_memory_usage returns actual memory consumption
5. All error paths return appropriate error codes
</success_criteria>

<output>
After completion, create `.planning/phases/01-cpp-core-llama-integration/01-02-SUMMARY.md`
</output>
