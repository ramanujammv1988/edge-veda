---
phase: 01-cpp-core-llama-integration
plan: 04
type: execute
wave: 4
depends_on: ["01-03"]
files_modified:
  - core/tests/test_inference.cpp
  - core/tests/CMakeLists.txt
  - scripts/test-inference.sh
autonomous: false

must_haves:
  truths:
    - "C++ engine successfully loads GGUF models"
    - "C++ engine generates coherent text output from prompts"
    - "Metal GPU acceleration achieves >10 tokens/second"
    - "Memory usage stays within configured 1.2GB limit"
  artifacts:
    - path: "core/tests/test_inference.cpp"
      provides: "Inference smoke test"
      min_lines: 100
    - path: "scripts/test-inference.sh"
      provides: "Test runner script"
  key_links:
    - from: "core/tests/test_inference.cpp"
      to: "core/include/edge_veda.h"
      via: "#include"
      pattern: '#include.*edge_veda\\.h'
---

<objective>
Create a smoke test that verifies the C++ core can load a model and generate text, with performance measurement.

Purpose: This validates that all the prior work (llama.cpp integration, engine implementation, iOS build) actually produces working inference. Without this test, we cannot verify the phase success criteria.

Output: Test program that loads a GGUF model, generates text, and reports performance metrics.
</objective>

<execution_context>
@/Users/ram/.claude/get-shit-done/workflows/execute-plan.md
@/Users/ram/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/phases/01-cpp-core-llama-integration/01-03-SUMMARY.md
@core/include/edge_veda.h
@core/src/engine.cpp
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create inference test program</name>
  <files>
    core/tests/test_inference.cpp
    core/tests/CMakeLists.txt
  </files>
  <action>
Create a test program that exercises the complete inference pipeline.

**Create core/tests/CMakeLists.txt:**
```cmake
# Edge Veda Core Tests
cmake_minimum_required(VERSION 3.15)

# Test inference executable
add_executable(test_inference test_inference.cpp)

target_link_libraries(test_inference PRIVATE edge_veda_static)

target_include_directories(test_inference PRIVATE
    ${CMAKE_SOURCE_DIR}/include
)

# Copy test to output directory
set_target_properties(test_inference PROPERTIES
    RUNTIME_OUTPUT_DIRECTORY ${CMAKE_BINARY_DIR}/tests
)
```

**Create core/tests/test_inference.cpp:**
```cpp
/**
 * @file test_inference.cpp
 * @brief Smoke test for Edge Veda inference engine
 *
 * Tests: model loading, text generation, memory tracking, cleanup
 * Reports: tokens/sec, time to first token, memory usage
 */

#include "edge_veda.h"
#include <cstdio>
#include <cstdlib>
#include <cstring>
#include <chrono>
#include <string>

// ANSI colors for output
#define GREEN "\033[32m"
#define RED "\033[31m"
#define YELLOW "\033[33m"
#define RESET "\033[0m"

void print_pass(const char* test) {
    printf(GREEN "[PASS]" RESET " %s\n", test);
}

void print_fail(const char* test, const char* reason) {
    printf(RED "[FAIL]" RESET " %s: %s\n", test, reason);
}

void print_info(const char* msg) {
    printf(YELLOW "[INFO]" RESET " %s\n", msg);
}

int main(int argc, char** argv) {
    printf("\n=== Edge Veda Inference Test ===\n\n");

    // Check for model path argument
    if (argc < 2) {
        printf("Usage: %s <model.gguf> [prompt]\n", argv[0]);
        printf("\nExample:\n");
        printf("  %s ./models/llama-3.2-1b-q4_k_m.gguf\n", argv[0]);
        printf("  %s ./models/llama-3.2-1b-q4_k_m.gguf \"What is 2+2?\"\n", argv[0]);
        return 1;
    }

    const char* model_path = argv[1];
    const char* prompt = (argc > 2) ? argv[2] : "Hello, I am a helpful AI assistant.";

    int failures = 0;
    int passes = 0;

    // Test 1: Version
    printf("--- Version Check ---\n");
    const char* version = ev_version();
    if (version && strlen(version) > 0) {
        printf("SDK Version: %s\n", version);
        print_pass("Version check");
        passes++;
    } else {
        print_fail("Version check", "No version string");
        failures++;
    }

    // Test 2: Backend detection
    printf("\n--- Backend Detection ---\n");
    ev_backend_t backend = ev_detect_backend();
    printf("Detected backend: %s\n", ev_backend_name(backend));
    if (ev_is_backend_available(backend)) {
        print_pass("Backend available");
        passes++;
    } else {
        print_fail("Backend available", "No backend available");
        failures++;
    }

    // Test 3: Model loading
    printf("\n--- Model Loading ---\n");
    printf("Model path: %s\n", model_path);

    ev_config config;
    ev_config_default(&config);
    config.model_path = model_path;
    config.backend = EV_BACKEND_AUTO;
    config.context_size = 2048;
    config.gpu_layers = -1;  // All layers to GPU
    config.memory_limit_bytes = 1200 * 1024 * 1024;  // 1.2GB limit

    auto load_start = std::chrono::high_resolution_clock::now();

    ev_error_t error;
    ev_context ctx = ev_init(&config, &error);

    auto load_end = std::chrono::high_resolution_clock::now();
    auto load_ms = std::chrono::duration_cast<std::chrono::milliseconds>(load_end - load_start).count();

    if (ctx && error == EV_SUCCESS) {
        printf("Model loaded in %lld ms\n", (long long)load_ms);
        print_pass("Model loading");
        passes++;
    } else {
        printf("Error: %s (%d)\n", ev_error_string(error), error);
        if (ctx) {
            printf("Last error: %s\n", ev_get_last_error(ctx));
        }
        print_fail("Model loading", ev_error_string(error));
        failures++;
        return 1;  // Can't continue without model
    }

    // Test 4: Model info
    printf("\n--- Model Info ---\n");
    ev_model_info info;
    if (ev_get_model_info(ctx, &info) == EV_SUCCESS) {
        printf("Name: %s\n", info.name ? info.name : "unknown");
        printf("Parameters: %llu\n", (unsigned long long)info.num_parameters);
        printf("Context length: %d\n", info.context_length);
        printf("Embedding dim: %d\n", info.embedding_dim);
        printf("Layers: %d\n", info.num_layers);
        print_pass("Model info");
        passes++;
    } else {
        print_fail("Model info", "Failed to get model info");
        failures++;
    }

    // Test 5: Memory tracking
    printf("\n--- Memory Usage ---\n");
    ev_memory_stats mem_stats;
    if (ev_get_memory_usage(ctx, &mem_stats) == EV_SUCCESS) {
        printf("Current: %.2f MB\n", mem_stats.current_bytes / (1024.0 * 1024.0));
        printf("Model: %.2f MB\n", mem_stats.model_bytes / (1024.0 * 1024.0));
        printf("Context: %.2f MB\n", mem_stats.context_bytes / (1024.0 * 1024.0));
        printf("Limit: %.2f MB\n", mem_stats.limit_bytes / (1024.0 * 1024.0));

        if (mem_stats.current_bytes < mem_stats.limit_bytes || mem_stats.limit_bytes == 0) {
            print_pass("Memory under limit");
            passes++;
        } else {
            print_fail("Memory under limit", "Exceeds configured limit");
            failures++;
        }
    } else {
        print_fail("Memory tracking", "Failed to get memory stats");
        failures++;
    }

    // Test 6: Text generation
    printf("\n--- Text Generation ---\n");
    printf("Prompt: \"%s\"\n\n", prompt);

    ev_generation_params gen_params;
    ev_generation_params_default(&gen_params);
    gen_params.max_tokens = 50;
    gen_params.temperature = 0.7f;
    gen_params.top_p = 0.9f;
    gen_params.top_k = 40;

    char* output = nullptr;

    auto gen_start = std::chrono::high_resolution_clock::now();

    ev_error_t gen_error = ev_generate(ctx, prompt, &gen_params, &output);

    auto gen_end = std::chrono::high_resolution_clock::now();
    auto gen_ms = std::chrono::duration_cast<std::chrono::milliseconds>(gen_end - gen_start).count();

    if (gen_error == EV_SUCCESS && output) {
        printf("Generated text:\n%s\n\n", output);

        // Calculate approximate tokens (rough: ~4 chars per token)
        size_t output_len = strlen(output);
        int approx_tokens = (int)(output_len / 4.0);
        float tokens_per_sec = (gen_ms > 0) ? (approx_tokens * 1000.0f / gen_ms) : 0;

        printf("Generation time: %lld ms\n", (long long)gen_ms);
        printf("Output length: %zu chars\n", output_len);
        printf("Approx tokens: %d\n", approx_tokens);
        printf("Approx speed: %.1f tok/sec\n", tokens_per_sec);

        if (output_len > 0) {
            print_pass("Text generation");
            passes++;
        } else {
            print_fail("Text generation", "Empty output");
            failures++;
        }

        // Check performance target (10 tok/sec)
        if (tokens_per_sec >= 10.0f) {
            print_pass("Performance target (>10 tok/sec)");
            passes++;
        } else {
            char perf_msg[100];
            snprintf(perf_msg, sizeof(perf_msg), "Only %.1f tok/sec (target: >10)", tokens_per_sec);
            print_fail("Performance target", perf_msg);
            failures++;
        }

        ev_free_string(output);
    } else {
        printf("Generation error: %s\n", ev_error_string(gen_error));
        if (ctx) {
            printf("Last error: %s\n", ev_get_last_error(ctx));
        }
        print_fail("Text generation", ev_error_string(gen_error));
        failures++;
    }

    // Test 7: Reset
    printf("\n--- Context Reset ---\n");
    if (ev_reset(ctx) == EV_SUCCESS) {
        print_pass("Context reset");
        passes++;
    } else {
        print_fail("Context reset", "Reset failed");
        failures++;
    }

    // Test 8: Cleanup
    printf("\n--- Cleanup ---\n");
    ev_free(ctx);
    print_pass("Cleanup");
    passes++;

    // Summary
    printf("\n=== Test Summary ===\n");
    printf("Passed: %d\n", passes);
    printf("Failed: %d\n", failures);

    if (failures == 0) {
        printf(GREEN "\nAll tests passed!\n" RESET);
        return 0;
    } else {
        printf(RED "\n%d test(s) failed.\n" RESET, failures);
        return 1;
    }
}
```

**Update core/CMakeLists.txt to include tests:**
Add at the end of the file:
```cmake
# Tests (optional)
if(EDGE_VEDA_BUILD_TESTS)
    enable_testing()
    add_subdirectory(tests)
endif()
```

(This might already exist - check first)

**Why this test:**
- Exercises complete inference pipeline
- Reports performance metrics (critical for phase success)
- Validates memory tracking
- Provides clear pass/fail output
  </action>
  <verify>
```bash
# Files exist
ls core/tests/test_inference.cpp
ls core/tests/CMakeLists.txt

# Test includes edge_veda.h
grep -c '#include "edge_veda.h"' core/tests/test_inference.cpp  # >= 1

# Test has generation code
grep -c "ev_generate" core/tests/test_inference.cpp  # >= 1
```
  </verify>
  <done>
- Test file created at core/tests/test_inference.cpp
- CMakeLists.txt created for tests
- Test exercises model loading, generation, cleanup
- Test reports performance metrics
  </done>
</task>

<task type="auto">
  <name>Task 2: Create test runner script</name>
  <files>
    scripts/test-inference.sh
  </files>
  <action>
Create a script to build and run the inference test.

**Create scripts/test-inference.sh:**
```bash
#!/bin/bash
set -e

# Test Edge Veda inference on macOS (not iOS - needs device)
# Usage: ./scripts/test-inference.sh <model.gguf> [prompt]

SCRIPT_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"
PROJECT_ROOT="$(dirname "$SCRIPT_DIR")"
CORE_DIR="$PROJECT_ROOT/core"
BUILD_DIR="$PROJECT_ROOT/build/macos-test"

echo "=== Edge Veda Inference Test ==="

# Check for model path
if [ -z "$1" ]; then
    echo "Usage: $0 <model.gguf> [prompt]"
    echo ""
    echo "Example:"
    echo "  $0 ~/models/llama-3.2-1b-q4_k_m.gguf"
    echo "  $0 ~/models/llama-3.2-1b-q4_k_m.gguf 'What is 2+2?'"
    echo ""
    echo "You can download a test model from:"
    echo "  https://huggingface.co/bartowski/Llama-3.2-1B-Instruct-GGUF"
    exit 1
fi

MODEL_PATH="$1"
PROMPT="${2:-Hello, I am a helpful AI assistant.}"

# Verify model exists
if [ ! -f "$MODEL_PATH" ]; then
    echo "ERROR: Model file not found: $MODEL_PATH"
    exit 1
fi

# Initialize submodules if needed
if [ ! -f "$CORE_DIR/third_party/llama.cpp/CMakeLists.txt" ]; then
    echo "Initializing git submodules..."
    cd "$PROJECT_ROOT"
    git submodule update --init --recursive
fi

# Build for macOS (native, for testing)
echo ""
echo "Building test executable for macOS..."
mkdir -p "$BUILD_DIR"

cmake -B "$BUILD_DIR" \
    -S "$CORE_DIR" \
    -DCMAKE_BUILD_TYPE=Release \
    -DEDGE_VEDA_BUILD_SHARED=OFF \
    -DEDGE_VEDA_BUILD_STATIC=ON \
    -DEDGE_VEDA_BUILD_TESTS=ON \
    -DEDGE_VEDA_ENABLE_METAL=ON

cmake --build "$BUILD_DIR" --config Release -j$(sysctl -n hw.ncpu)

# Find test executable
TEST_EXE="$BUILD_DIR/tests/test_inference"
if [ ! -f "$TEST_EXE" ]; then
    TEST_EXE=$(find "$BUILD_DIR" -name "test_inference" -type f | head -1)
fi

if [ ! -f "$TEST_EXE" ]; then
    echo "ERROR: Test executable not found"
    find "$BUILD_DIR" -name "test_*" -ls
    exit 1
fi

echo ""
echo "Running test..."
echo "Model: $MODEL_PATH"
echo "Prompt: $PROMPT"
echo ""

# Run the test
"$TEST_EXE" "$MODEL_PATH" "$PROMPT"
```

Make executable:
```bash
chmod +x scripts/test-inference.sh
```

**Note:** This test runs on macOS (the development machine), not iOS. iOS testing requires:
1. A physical iOS device
2. The Flutter integration (Phase 2+)
3. Code signing and provisioning

The macOS test validates:
- llama.cpp integration works
- Model loading works
- Generation works
- Performance baseline (macOS has similar Metal backend)
  </action>
  <verify>
```bash
# Script exists and is executable
ls -la scripts/test-inference.sh
file scripts/test-inference.sh

# Script has key commands
grep -c "cmake" scripts/test-inference.sh  # >= 2
grep -c "test_inference" scripts/test-inference.sh  # >= 1
```
  </verify>
  <done>
- Test runner script created
- Script is executable
- Script builds and runs test
  </done>
</task>

<task type="checkpoint:human-verify" gate="blocking">
  <what-built>
Complete C++ inference engine with llama.cpp integration:
- llama.cpp submodule added
- CMake configured for iOS Metal builds
- engine.cpp implements real inference
- XCFramework built for iOS device/simulator
- Test program validates generation
  </what-built>
  <how-to-verify>
**IMPORTANT:** This test requires a GGUF model file. You need to download one first.

**Step 1: Download a test model**
```bash
# Create models directory
mkdir -p ~/models

# Download Llama 3.2 1B (Q4_K_M quantization, ~750MB)
# Option A: Use huggingface-cli
pip install huggingface_hub
huggingface-cli download bartowski/Llama-3.2-1B-Instruct-GGUF \
  Llama-3.2-1B-Instruct-Q4_K_M.gguf \
  --local-dir ~/models

# Option B: Direct download from browser
# https://huggingface.co/bartowski/Llama-3.2-1B-Instruct-GGUF/tree/main
# Download: Llama-3.2-1B-Instruct-Q4_K_M.gguf
```

**Step 2: Run the inference test**
```bash
cd /Users/ram/Documents/explore/edge
./scripts/test-inference.sh ~/models/Llama-3.2-1B-Instruct-Q4_K_M.gguf "What is 2+2?"
```

**Expected results:**
1. Build completes without errors
2. Model loads successfully
3. Text is generated (should be coherent)
4. Performance >= 10 tok/sec on M1+ Mac
5. All tests pass (green)

**Verification criteria:**
- [ ] Model loads without error
- [ ] Generated text makes sense (not garbage)
- [ ] Performance reported
- [ ] Memory usage reported
- [ ] All tests pass

**If test fails:**
- Check error messages
- Verify model file exists and is valid GGUF
- Check Metal is available (macOS 12+)
  </how-to-verify>
  <resume-signal>Type "approved" if inference works and generates coherent text, or describe any issues encountered.</resume-signal>
</task>

</tasks>

<verification>
After all tasks complete (including human verification):

1. **Test program compiles and runs:**
   ```bash
   ./scripts/test-inference.sh <model.gguf>
   # Should complete with "All tests passed!"
   ```

2. **Performance meets target:**
   - Output shows >= 10 tok/sec (on Mac)
   - Note: iOS performance will be verified in Phase 3 with real device

3. **Generated text is coherent:**
   - Human verification that output makes sense
   - Not random garbage or repeated tokens
</verification>

<success_criteria>
1. C++ engine loads GGUF models successfully
2. C++ engine generates coherent text from prompts
3. Metal GPU acceleration achieves >= 10 tok/sec on development machine
4. Human verified that generated text is sensible
5. All test assertions pass
</success_criteria>

<output>
After completion, create `.planning/phases/01-cpp-core-llama-integration/01-04-SUMMARY.md`
</output>
