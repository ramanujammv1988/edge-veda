---
phase: 01-cpp-core-llama-integration
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - core/third_party/llama.cpp
  - core/CMakeLists.txt
  - .gitmodules
autonomous: true

must_haves:
  truths:
    - "llama.cpp submodule exists at pinned commit"
    - "CMake configures without errors with GGML_METAL ON"
    - "Desktop SIMD disabled (AVX/AVX2/FMA/F16C OFF)"
  artifacts:
    - path: "core/third_party/llama.cpp/CMakeLists.txt"
      provides: "llama.cpp build system"
    - path: "core/CMakeLists.txt"
      provides: "Updated build config with llama.cpp integration"
      contains: "GGML_METAL ON"
    - path: ".gitmodules"
      provides: "Git submodule configuration"
      contains: "llama.cpp"
  key_links:
    - from: "core/CMakeLists.txt"
      to: "core/third_party/llama.cpp"
      via: "add_subdirectory"
      pattern: "add_subdirectory.*third_party/llama\\.cpp"
---

<objective>
Add llama.cpp as a git submodule and configure CMake for iOS Metal builds with binary size controls.

Purpose: This is the foundation - without llama.cpp integrated and properly configured, no inference is possible. Correct CMake configuration is critical to avoid binary bloat and ensure Metal GPU acceleration works.

Output: llama.cpp submodule at pinned commit, CMakeLists.txt configured for iOS/Metal build with desktop SIMD disabled.
</objective>

<execution_context>
@/Users/ram/.claude/get-shit-done/workflows/execute-plan.md
@/Users/ram/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@core/CMakeLists.txt
</context>

<tasks>

<task type="auto">
  <name>Task 1: Add llama.cpp as git submodule</name>
  <files>
    core/third_party/llama.cpp
    .gitmodules
  </files>
  <action>
Add llama.cpp as a git submodule at a stable pinned commit.

1. Create the third_party directory if it doesn't exist:
   ```bash
   mkdir -p core/third_party
   ```

2. Add the submodule at a known stable commit (use b4658 tag or recent stable):
   ```bash
   cd core
   git submodule add https://github.com/ggerganov/llama.cpp.git third_party/llama.cpp
   cd third_party/llama.cpp
   git checkout b4658  # Or latest stable tag - verify first
   cd ../../..
   ```

3. Record the exact commit hash used for reproducibility.

**Important:** Choose a commit from the last 2-3 weeks that has the current ggml API (with GGML_METAL, not LLAMA_METAL - the API changed). Check the llama.cpp releases page for a stable tag.

**Why pinned commit:** llama.cpp API changes frequently. Pinning ensures reproducible builds.
  </action>
  <verify>
Run these commands to verify:
```bash
# Submodule exists and has content
ls core/third_party/llama.cpp/CMakeLists.txt

# .gitmodules has entry
grep -q "llama.cpp" .gitmodules && echo "Submodule configured"

# Check commit is recorded
git submodule status core/third_party/llama.cpp
```
  </verify>
  <done>
- llama.cpp directory exists with CMakeLists.txt
- .gitmodules contains llama.cpp entry
- Submodule is at a specific commit (not floating HEAD)
  </done>
</task>

<task type="auto">
  <name>Task 2: Update CMakeLists.txt for llama.cpp integration</name>
  <files>
    core/CMakeLists.txt
  </files>
  <action>
Update core/CMakeLists.txt to properly integrate llama.cpp with iOS Metal support and binary size controls.

**Critical configuration requirements:**

1. **llama.cpp build options** (add before add_subdirectory):
```cmake
if(EXISTS "${CMAKE_CURRENT_SOURCE_DIR}/third_party/llama.cpp/CMakeLists.txt")
    # Disable tests/examples/server (reduces build time and size)
    set(LLAMA_BUILD_TESTS OFF CACHE BOOL "" FORCE)
    set(LLAMA_BUILD_EXAMPLES OFF CACHE BOOL "" FORCE)
    set(LLAMA_BUILD_SERVER OFF CACHE BOOL "" FORCE)
    set(BUILD_SHARED_LIBS OFF CACHE BOOL "" FORCE)

    # Metal configuration for iOS/macOS
    if(IOS OR MACOS)
        set(GGML_METAL ON CACHE BOOL "" FORCE)
        set(GGML_METAL_EMBED_LIBRARY ON CACHE BOOL "" FORCE)
    endif()

    # Accelerate framework for Apple platforms
    if(APPLE)
        set(GGML_ACCELERATE ON CACHE BOOL "" FORCE)
    endif()

    # CRITICAL: Disable desktop SIMD to prevent binary bloat
    set(GGML_NATIVE OFF CACHE BOOL "" FORCE)
    set(GGML_AVX OFF CACHE BOOL "" FORCE)
    set(GGML_AVX2 OFF CACHE BOOL "" FORCE)
    set(GGML_AVX512 OFF CACHE BOOL "" FORCE)
    set(GGML_AVX512_VBMI OFF CACHE BOOL "" FORCE)
    set(GGML_AVX512_VNNI OFF CACHE BOOL "" FORCE)
    set(GGML_FMA OFF CACHE BOOL "" FORCE)
    set(GGML_F16C OFF CACHE BOOL "" FORCE)

    # Keep ARM NEON (automatic on iOS arm64)

    add_subdirectory(third_party/llama.cpp)
    set(LLAMA_CPP_AVAILABLE TRUE)
else()
    message(WARNING "llama.cpp not found - building without LLM support")
    set(LLAMA_CPP_AVAILABLE FALSE)
endif()
```

2. **Update linking** - change target_link_libraries to link against `ggml` and `llama`:
```cmake
if(LLAMA_CPP_AVAILABLE)
    target_link_libraries(${target_name} PRIVATE llama ggml)
    target_compile_definitions(${target_name} PRIVATE EDGE_VEDA_LLAMA_ENABLED)
    target_include_directories(${target_name} PRIVATE
        ${CMAKE_CURRENT_SOURCE_DIR}/third_party/llama.cpp/include
        ${CMAKE_CURRENT_SOURCE_DIR}/third_party/llama.cpp/ggml/include
    )
endif()
```

3. **iOS-specific flags** - ensure these are set:
```cmake
if(IOS)
    set(CMAKE_OSX_DEPLOYMENT_TARGET "13.0" CACHE STRING "")
    set(CMAKE_XCODE_ATTRIBUTE_ONLY_ACTIVE_ARCH NO)
    set(CMAKE_XCODE_ATTRIBUTE_ENABLE_BITCODE NO)
    set(CMAKE_OSX_ARCHITECTURES "arm64" CACHE STRING "")

    # Link required frameworks
    find_library(METAL_FRAMEWORK Metal REQUIRED)
    find_library(METALKIT_FRAMEWORK MetalKit REQUIRED)
    find_library(FOUNDATION_FRAMEWORK Foundation REQUIRED)
    find_library(ACCELERATE_FRAMEWORK Accelerate REQUIRED)
endif()
```

4. **Enable LTO for release builds** (reduces binary size):
```cmake
if(CMAKE_BUILD_TYPE STREQUAL "Release")
    set(CMAKE_INTERPROCEDURAL_OPTIMIZATION TRUE)
endif()
```

**Why these settings:**
- GGML_NATIVE OFF: Prevents auto-detection of host CPU features (we're cross-compiling)
- AVX/AVX2/FMA/F16C OFF: x86 instructions not needed on arm64, would bloat binary
- GGML_METAL_EMBED_LIBRARY ON: Embeds Metal shaders in library (required for iOS)
- LTO: Link-time optimization reduces final binary size by ~10-20%
  </action>
  <verify>
Run CMake configuration in dry-run mode to verify no errors:
```bash
cd core
cmake -B build-test \
  -DCMAKE_TOOLCHAIN_FILE=cmake/ios.toolchain.cmake \
  -DPLATFORM=OS64 \
  -DCMAKE_BUILD_TYPE=Release \
  -DEDGE_VEDA_BUILD_SHARED=OFF \
  -DEDGE_VEDA_BUILD_STATIC=ON \
  2>&1 | tee cmake_output.txt

# Check for critical settings in output
grep -E "(GGML_METAL|LLAMA_CPP_AVAILABLE)" cmake_output.txt

# Clean up test build
rm -rf build-test cmake_output.txt
```

**Verify all critical CMake flags are present in CMakeLists.txt:**
```bash
# Check GGML_METAL_EMBED_LIBRARY ON (critical for iOS Metal shaders)
grep -q "GGML_METAL_EMBED_LIBRARY ON" core/CMakeLists.txt && echo "PASS: GGML_METAL_EMBED_LIBRARY ON" || echo "FAIL: Missing GGML_METAL_EMBED_LIBRARY ON"

# Check desktop SIMD is disabled
grep -q "GGML_AVX OFF" core/CMakeLists.txt && echo "PASS: GGML_AVX OFF" || echo "FAIL: Missing GGML_AVX OFF"
grep -q "GGML_AVX2 OFF" core/CMakeLists.txt && echo "PASS: GGML_AVX2 OFF" || echo "FAIL: Missing GGML_AVX2 OFF"
grep -q "GGML_FMA OFF" core/CMakeLists.txt && echo "PASS: GGML_FMA OFF" || echo "FAIL: Missing GGML_FMA OFF"
grep -q "GGML_F16C OFF" core/CMakeLists.txt && echo "PASS: GGML_F16C OFF" || echo "FAIL: Missing GGML_F16C OFF"

# Check both llama and ggml are linked
grep -E "target_link_libraries.*llama.*ggml|target_link_libraries.*ggml.*llama" core/CMakeLists.txt && echo "PASS: Both llama and ggml linked" || echo "FAIL: Missing ggml in target_link_libraries"

# Check LTO is configured for release
grep -q "CMAKE_INTERPROCEDURAL_OPTIMIZATION" core/CMakeLists.txt && echo "PASS: LTO configured" || echo "FAIL: Missing LTO configuration"
```
  </verify>
  <done>
- CMake configures without errors
- Output shows GGML_METAL: ON
- Output shows LLAMA_CPP_AVAILABLE: TRUE
- GGML_METAL_EMBED_LIBRARY ON is present
- All SIMD flags (AVX, AVX2, FMA, F16C) are OFF
- Both llama and ggml are in target_link_libraries
- LTO is configured for Release builds
  </done>
</task>

<task type="auto">
  <name>Task 3: Commit llama.cpp integration</name>
  <files>
    .gitmodules
    core/CMakeLists.txt
  </files>
  <action>
Commit the llama.cpp submodule and CMake changes.

```bash
git add .gitmodules core/third_party/llama.cpp core/CMakeLists.txt
git commit -m "feat(01-01): add llama.cpp submodule with iOS Metal configuration

- Add llama.cpp as git submodule at pinned commit
- Configure GGML_METAL ON for iOS GPU acceleration
- Disable desktop SIMD (AVX/AVX2/FMA) to prevent binary bloat
- Enable GGML_METAL_EMBED_LIBRARY for iOS shader embedding
- Add LTO for release builds"
```

**Do not push** - just commit locally.
  </action>
  <verify>
```bash
git log -1 --oneline
git status
```
  </verify>
  <done>
- Commit exists with llama.cpp integration
- Working directory is clean
  </done>
</task>

</tasks>

<verification>
After all tasks complete:

1. **Submodule integrity:**
   ```bash
   git submodule status
   # Should show commit hash for core/third_party/llama.cpp
   ```

2. **CMake configuration test:**
   ```bash
   cd core && cmake -B build-verify \
     -DCMAKE_TOOLCHAIN_FILE=cmake/ios.toolchain.cmake \
     -DPLATFORM=OS64 \
     -DCMAKE_BUILD_TYPE=Release \
     -DEDGE_VEDA_BUILD_STATIC=ON \
     -DEDGE_VEDA_BUILD_SHARED=OFF
   # Must complete without errors
   rm -rf build-verify
   ```

3. **Key configuration flags present in CMakeLists.txt:**
   ```bash
   grep -c "GGML_METAL ON" core/CMakeLists.txt  # Should be >= 1
   grep -c "GGML_AVX OFF" core/CMakeLists.txt   # Should be >= 1
   grep -c "GGML_METAL_EMBED_LIBRARY ON" core/CMakeLists.txt  # Should be >= 1
   grep -E "target_link_libraries.*ggml" core/CMakeLists.txt  # Should match
   ```
</verification>

<success_criteria>
1. llama.cpp exists as submodule at core/third_party/llama.cpp
2. CMake configures successfully for iOS arm64 with Metal
3. No desktop SIMD options enabled
4. GGML_METAL_EMBED_LIBRARY ON is set
5. Both llama and ggml are linked
6. Changes committed to git
</success_criteria>

<output>
After completion, create `.planning/phases/01-cpp-core-llama-integration/01-01-SUMMARY.md`
</output>
