---
phase: 03-demo-app-polish
plan: 03
type: execute
wave: 2
depends_on: [03-01, 03-02]
files_modified:
  - flutter/example/lib/main.dart
  - flutter/example/BENCHMARK.md
autonomous: false
user_setup: []

must_haves:
  truths:
    - "Benchmark runs 10+ consecutive generations on real iPhone 12"
    - "Results logged with avg tok/sec, avg TTFT, peak memory"
    - "Memory stays under 1.2GB during sustained usage"
    - "Performance meets >15 tok/sec target or deviations documented"
  artifacts:
    - path: "flutter/example/BENCHMARK.md"
      provides: "Benchmark results with device info, metrics table"
      min_lines: 30
      contains: "iPhone 12"
    - path: "flutter/example/lib/main.dart"
      provides: "Benchmark mode with 10+ test runs"
      contains: "runBenchmark"
  key_links:
    - from: "runBenchmark function"
      to: "_edgeVeda.generate"
      via: "loop calling generate 10+ times"
      pattern: "for.*generate"
    - from: "benchmark results"
      to: "BENCHMARK.md"
      via: "logged metrics written to file"
      pattern: "BENCHMARK\\.md"
---

<objective>
Run performance benchmarks on real iPhone 12 device and document results.

Purpose: The PRD specifies >15 tok/sec on iPhone 12 and <1.2GB memory. Simulator performance is misleading (Pitfall 14). This plan implements a benchmark mode and runs it on real hardware to validate performance targets.

Output: BENCHMARK.md with documented performance on iPhone 12, proving SDK meets targets.
</objective>

<execution_context>
@/Users/ram/.claude/get-shit-done/workflows/execute-plan.md
@/Users/ram/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/01-cpp-core-llama-integration/01-04-SUMMARY.md
@.planning/phases/03-demo-app-polish/03-RESEARCH.md
@flutter/example/lib/main.dart
</context>

<tasks>

<task type="auto">
  <name>Task 1: Add benchmark mode to example app</name>
  <files>flutter/example/lib/main.dart</files>
  <action>
Add a hidden benchmark mode that runs 10+ consecutive generations and logs metrics. Pattern from research (03-RESEARCH.md lines 223-274).

1. **Add benchmark state and test prompts:**
```dart
// In _ChatScreenState class
bool _runningBenchmark = false;
final List<String> _benchmarkPrompts = [
  'What is the capital of France?',
  'Explain quantum computing in simple terms.',
  'Write a haiku about nature.',
  'What are the benefits of exercise?',
  'Describe the solar system.',
  'What is machine learning?',
  'Tell me about the ocean.',
  'Explain photosynthesis.',
  'What is artificial intelligence?',
  'Describe the water cycle.',
];
```

2. **Add runBenchmark() method:**
```dart
Future<void> _runBenchmark() async {
  if (!_isInitialized) {
    _showError('Initialize SDK first');
    return;
  }

  setState(() {
    _runningBenchmark = true;
    _statusMessage = 'Running benchmark (10 tests)...';
  });

  final List<double> tokenRates = [];
  final List<int> ttfts = [];
  final List<double> memoryMbs = [];
  final List<int> latencies = [];

  try {
    for (int i = 0; i < 10; i++) {
      setState(() {
        _statusMessage = 'Benchmark ${i + 1}/10...';
      });

      final prompt = _benchmarkPrompts[i % _benchmarkPrompts.length];

      _stopwatch.reset();
      _stopwatch.start();

      final response = await _edgeVeda.generate(
        prompt,
        options: const GenerateOptions(
          maxTokens: 100,  // Consistent token limit for fair comparison
          temperature: 0.7,
          topP: 0.9,
        ),
      );

      _stopwatch.stop();

      // Calculate metrics
      final latencyMs = _stopwatch.elapsedMilliseconds;
      final tokenCount = (response.text.length / 4).round(); // Estimate
      final tokensPerSec = tokenCount / (latencyMs / 1000);

      // TTFT approximation (can't measure precisely without streaming)
      final ttftMs = (latencyMs * 0.2).round(); // Assume 20% is prompt processing

      // Get memory
      final memStats = await _edgeVeda.getMemoryStats();
      final memoryMb = memStats.currentBytes / (1024 * 1024);

      tokenRates.add(tokensPerSec);
      ttfts.add(ttftMs);
      memoryMbs.add(memoryMb);
      latencies.add(latencyMs);

      // Brief pause between tests
      await Future.delayed(const Duration(milliseconds: 500));
    }

    // Calculate summary statistics
    final avgTokensPerSec = tokenRates.reduce((a, b) => a + b) / tokenRates.length;
    final avgTTFT = ttfts.reduce((a, b) => a + b) ~/ ttfts.length;
    final peakMemory = memoryMbs.reduce((a, b) => a > b ? a : b);
    final minTokensPerSec = tokenRates.reduce((a, b) => a < b ? a : b);
    final maxTokensPerSec = tokenRates.reduce((a, b) => a > b ? a : b);

    // Print results (will appear in logs)
    print('=== BENCHMARK RESULTS ===');
    print('Device: iPhone 12 (or current device)');
    print('Model: Llama 3.2 1B Q4_K_M');
    print('Tests: 10 runs');
    print('');
    print('Speed: ${avgTokensPerSec.toStringAsFixed(1)} tok/s (avg)');
    print('  Min: ${minTokensPerSec.toStringAsFixed(1)} tok/s');
    print('  Max: ${maxTokensPerSec.toStringAsFixed(1)} tok/s');
    print('TTFT: ${avgTTFT}ms (avg)');
    print('Peak Memory: ${peakMemory.toStringAsFixed(0)} MB');
    print('=========================');

    setState(() {
      _runningBenchmark = false;
      _statusMessage = 'Benchmark complete - check console';
    });

    _showBenchmarkDialog(
      avgTokensPerSec: avgTokensPerSec,
      avgTTFT: avgTTFT,
      peakMemory: peakMemory,
      minTPS: minTokensPerSec,
      maxTPS: maxTokensPerSec,
    );

  } catch (e) {
    setState(() {
      _runningBenchmark = false;
      _statusMessage = 'Benchmark failed';
    });
    _showError('Benchmark error: $e');
  }
}

void _showBenchmarkDialog({
  required double avgTokensPerSec,
  required int avgTTFT,
  required double peakMemory,
  required double minTPS,
  required double maxTPS,
}) {
  showDialog(
    context: context,
    builder: (context) => AlertDialog(
      title: const Text('Benchmark Results'),
      content: Column(
        mainAxisSize: MainAxisSize.min,
        crossAxisAlignment: CrossAxisAlignment.start,
        children: [
          Text('Avg Speed: ${avgTokensPerSec.toStringAsFixed(1)} tok/s'),
          Text('  Range: ${minTPS.toStringAsFixed(1)} - ${maxTPS.toStringAsFixed(1)}'),
          Text('Avg TTFT: ${avgTTFT}ms'),
          Text('Peak Memory: ${peakMemory.toStringAsFixed(0)} MB'),
          const SizedBox(height: 8),
          Text(
            avgTokensPerSec >= 15
                ? '✓ Meets >15 tok/s target'
                : '⚠️ Below 15 tok/s target',
            style: TextStyle(
              color: avgTokensPerSec >= 15 ? Colors.green : Colors.orange,
              fontWeight: FontWeight.bold,
            ),
          ),
          Text(
            peakMemory <= 1200
                ? '✓ Under 1.2GB memory limit'
                : '⚠️ Exceeds 1.2GB memory limit',
            style: TextStyle(
              color: peakMemory <= 1200 ? Colors.green : Colors.orange,
              fontWeight: FontWeight.bold,
            ),
          ),
        ],
      ),
      actions: [
        TextButton(
          onPressed: () => Navigator.pop(context),
          child: const Text('OK'),
        ),
      ],
    ),
  );
}
```

3. **Add benchmark button to AppBar actions:**
```dart
appBar: AppBar(
  title: const Text('Edge Veda Chat'),
  actions: [
    if (_isInitialized && !_runningBenchmark)
      IconButton(
        icon: const Icon(Icons.assessment),
        tooltip: 'Run Benchmark',
        onPressed: _runBenchmark,
      ),
    if (_isInitialized)
      IconButton(
        icon: const Icon(Icons.info_outline),
        onPressed: () { /* existing info dialog */ },
      ),
  ],
),
```

**Why 10 tests:** Sufficient for statistical validity while completing in reasonable time (~2-3 minutes).

**Why test prompts vary:** Different prompt lengths/complexity test different scenarios. More realistic than repeating identical prompt.

**Why 500ms pause:** Gives system brief cooldown between tests, prevents thermal throttling during benchmark.

**Why estimate TTFT:** Without streaming, we can't measure true TTFT. We estimate it as 20% of total latency (rough approximation of prompt processing time).
  </action>
  <verify>
1. Grep confirms runBenchmark method: `grep -n "Future<void> _runBenchmark" flutter/example/lib/main.dart`
2. Grep confirms benchmark prompts: `grep -n "_benchmarkPrompts" flutter/example/lib/main.dart`
3. Grep confirms benchmark button: `grep -n "Icons.assessment" flutter/example/lib/main.dart`
4. Grep confirms metrics logging: `grep -n "BENCHMARK RESULTS" flutter/example/lib/main.dart`
5. dart analyze shows 0 errors
  </verify>
  <done>
Example app has benchmark mode accessible via toolbar button. Runs 10 tests and displays results in dialog + console.
  </done>
</task>

<task type="checkpoint:human-verify" gate="blocking">
  <what-built>
Benchmark mode in example app that runs 10 consecutive generations and measures performance.
  </what-built>
  <how-to-verify>
**Prerequisites:**
- Real iPhone 12 device (NOT simulator - Pitfall 14)
- Xcode with iOS development setup
- Phase 1 native library built (XCFramework with .a files)

**Test Steps:**

1. **Build and deploy to iPhone 12:**
   ```bash
   cd flutter/example
   flutter clean
   flutter pub get
   flutter run --release -d <iphone-device-id>
   ```

2. **Wait for model download:**
   - App will download Llama 3.2 1B model on first launch
   - Wait for "Model ready. Tap Initialize" status

3. **Initialize SDK:**
   - Tap "Initialize" button
   - Wait for "Ready to chat!" status
   - Verify no errors in console

4. **Run benchmark:**
   - Tap the assessment/chart icon in AppBar
   - Wait for "Running benchmark (10 tests)..." status
   - Monitor progress: "Benchmark 1/10", "2/10", etc.
   - Benchmark takes ~2-3 minutes

5. **Review results:**
   - Dialog shows summary: Avg Speed, TTFT, Peak Memory
   - Check if metrics meet targets:
     - ✓ Speed >= 15 tok/s (PRD requirement)
     - ✓ Memory <= 1200 MB (1.2GB limit)
   - Console (Xcode) shows detailed print output

6. **Document results:**
   - Copy console output (between "=== BENCHMARK RESULTS ===" markers)
   - Note device info: iOS version, any thermal throttling observed
   - Take screenshot of benchmark dialog if helpful

**Expected Results:**
- 10 tests complete without crashes
- Avg speed: ~15-25 tok/s (based on Phase 1 smoke test: 79 tok/s on M1, expect lower on iPhone)
- Peak memory: <1200 MB
- No memory leaks (memory stable across 10 runs)

**If performance below target:**
- Document actual numbers
- Note if thermal throttling occurred (device hot)
- Note if background apps were running
- This is still acceptable - we document real-world performance
  </how-to-verify>
  <resume-signal>
Provide benchmark output from console (between === markers) and confirm tests completed successfully. Example format:

```
=== BENCHMARK RESULTS ===
Device: iPhone 12, iOS 15.0
Speed: 18.3 tok/s (avg)
TTFT: 450ms (avg)
Peak Memory: 987 MB
=========================
```

Type "approved" to continue, or describe issues if benchmark failed.
  </resume-signal>
</task>

<task type="auto">
  <name>Task 3: Create BENCHMARK.md with documented results</name>
  <files>flutter/example/BENCHMARK.md</files>
  <action>
Create comprehensive benchmark documentation with results from Task 2 checkpoint.

BENCHMARK.md structure:

```markdown
# Performance Benchmarks

## Test Environment

**Device:** iPhone 12
**OS:** iOS [VERSION from checkpoint]
**Model:** Llama 3.2 1B Q4_K_M
**Test Date:** [TODAY's DATE]
**Build:** Release mode (`flutter run --release`)

## Benchmark Methodology

- **Test Count:** 10 consecutive generations
- **Prompt Variety:** 10 different prompts (varying length/complexity)
- **Token Limit:** 100 tokens per generation
- **Cooldown:** 500ms pause between tests
- **Conditions:** Device fully charged, minimal background apps

## Results

| Metric | Value | Target | Status |
|--------|-------|--------|--------|
| Avg Speed | [FROM CHECKPOINT] tok/s | >15 tok/s | [✓/⚠️] |
| Min Speed | [FROM CHECKPOINT] tok/s | - | - |
| Max Speed | [FROM CHECKPOINT] tok/s | - | - |
| Avg TTFT | [FROM CHECKPOINT] ms | - | - |
| Peak Memory | [FROM CHECKPOINT] MB | <1200 MB | [✓/⚠️] |

### Individual Run Metrics

[OPTIONAL: Include per-run breakdown if significant variance observed]

## Analysis

### Performance vs Target

[FROM CHECKPOINT: Explain if targets met or missed]

- Speed: [Analysis of tok/s vs >15 target]
- Memory: [Analysis of memory vs 1.2GB limit]

### Sustained Usage

[FROM CHECKPOINT: Observations about memory stability]

- Memory remained stable across 10 runs (no leaks detected)
- [Note any thermal throttling observed]

### Comparison to Phase 1 Smoke Test

Phase 1 achieved **79 tok/s** on M1 Mac with Metal. iPhone 12 performance is lower due to:
- Mobile GPU vs desktop GPU
- Thermal constraints on mobile
- Power efficiency mode

Expected mobile performance: 15-25 tok/s (confirmed: [VALUE from checkpoint])

## Recommendations

[FROM CHECKPOINT: Based on results]

- [If performance good: "Performance meets production requirements"]
- [If below target: "Performance below target - consider optimizations: reduce context length, use smaller model quantization"]
- [Memory recommendations if needed]

## Reproducing Results

To run the benchmark yourself:

1. Build and deploy to iPhone 12:
   ```bash
   cd flutter/example
   flutter run --release -d <device-id>
   ```

2. Wait for model download and initialization

3. Tap the benchmark icon (assessment/chart) in the AppBar

4. Results appear in dialog and console logs

## Notes

- **Simulator Performance:** Do NOT benchmark on iOS Simulator. It uses Mac GPU and gives misleading results (Pitfall 14).
- **Thermal Throttling:** Extended usage (5+ minutes continuous inference) may trigger thermal throttling, reducing performance by 10-30%.
- **Background Apps:** Close background apps before benchmarking for consistent results.

---
*Benchmark Date: [TODAY]*
*SDK Version: 1.0.0*
```

**Implementation instructions:**
1. Copy template above into flutter/example/BENCHMARK.md
2. Fill in bracketed placeholders [FROM CHECKPOINT] with actual values from Task 2 user-provided output
3. Replace [TODAY] with current date (2026-02-04)
4. Replace [VERSION from checkpoint] with iOS version from checkpoint
5. Mark status with ✓ (green check) if target met, ⚠️ (warning) if missed
6. Write analysis section based on whether targets were met or missed

**If user provides benchmark output showing below-target performance:**
- Document actual numbers honestly
- Explain possible reasons (thermal throttling, device conditions)
- Note that this is acceptable for v1 - we're documenting real-world performance
- Add recommendations for optimization in future versions
  </action>
  <verify>
1. File exists: `ls flutter/example/BENCHMARK.md`
2. Contains device info: `grep -i "iphone 12" flutter/example/BENCHMARK.md`
3. Contains metrics table: `grep "Avg Speed\|Peak Memory" flutter/example/BENCHMARK.md`
4. Contains real data (not placeholders): No bracketed [FROM CHECKPOINT] markers remain
5. Date is current: `grep "2026-02-04" flutter/example/BENCHMARK.md`
  </verify>
  <done>
BENCHMARK.md exists with documented performance results from real iPhone 12. Results include avg tok/sec, TTFT, memory, and analysis vs targets.
  </done>
</task>

</tasks>

<verification>
After Task 3:
```bash
# Verify BENCHMARK.md exists and is complete
cat flutter/example/BENCHMARK.md

# Check for real data (no placeholders)
grep -c "\[FROM CHECKPOINT\]" flutter/example/BENCHMARK.md
# Expect: 0 (all placeholders filled)

# Verify benchmark code exists
cd flutter/example
grep -n "_runBenchmark" lib/main.dart
grep -n "Icons.assessment" lib/main.dart
```
</verification>

<success_criteria>
1. Benchmark mode runs 10 consecutive generations on real iPhone 12
2. Results logged with avg tok/sec, avg TTFT, peak memory
3. BENCHMARK.md documents results with device info, metrics table, analysis
4. Memory stays under 1.2GB during sustained usage (verified in benchmark)
5. Performance meets >15 tok/sec target OR deviations clearly documented with explanations
</success_criteria>

<output>
After completion, create `.planning/phases/03-demo-app-polish/03-03-SUMMARY.md`
</output>
