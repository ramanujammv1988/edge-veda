# **PRD: Edge veda SDK & Control Plane**

**Status:** Draft | **Timeline:** 12 Weeks | **Priority:** Flutter-First

LucidChart Architecture: [Lucidchart Document 4995e9b9-f2ce-446b-8433-09ccfef3bf94](https://lucid.app/lucidchart/4995e9b9-f2ce-446b-8433-09ccfef3bf94/edit?viewport_loc=-1719%2C-3112%2C2691%2C1605%2C0_0&invitationId=inv_95f8c4e2-0dcf-4a9a-b789-4cc0c246e217)

Detailed Roadmap and technical details: https://docs.google.com/spreadsheets/d/1b0RH7qFSIi1gQ1XrgTrXp_wLVs_G907yfCWQ5bSeXYQ/edit?gid=0#gid=0

**1. Executive Summary**
Modern AI relies on expensive, high-latency cloud APIs. **Edge veda** (meaning "Knowledge at the Edge") decentralizes this by providing a unified C++ inference engine wrapped in native and cross-platform SDKs. Our goal is to enable developers to run LLMs, Speech-to-Text, and Text-to-Speech directly on a user’s device with sub-200ms latency and zero server costs.

**2. Goals & Objectives**
• **Performance:** Achieve >15 tokens/sec for 1B parameter models on mid-range mobile devices.
• **Privacy:** 100% on-device processing; data never leaves the local environment.
• **Versatility:** Support 6 major platforms (Flutter iOS/Android/Web, Swift, Kotlin, React Native).
• **Connectivity:** Seamless operation in offline/airplane mode.

**3. Technical Architecture**
To meet the aggressive 12-week imeline, **Edge veda** uses a **"Hub and Spoke"** model. A single, highly optimized C++ core handles the heavy lifting, while thin wrapper layers provide an idiomatic experience for each language.

**The Core Stack**
• **Inference Engines:** `llama.cpp` (LLM), `whisper.cpp` (STT), `Kokoro-82M` (TTS).
• **Hardware Acceleration:** Metal (iOS), Vulkan/NNAPI (Android), WebGPU (Web).
• **Quantization:** GGUF/ONNX 4-bit formats for memory efficiency.

**4. Feature Requirements
A. Text-to-Text (T2T)**
• **Model:** Llama 3.2 1B (Primary), Phi-3.5 Mini (Reasoning).
• **Capabilities:** Streaming responses, JSON mode, system prompting.
• **API:** `generateStream(prompt, options)`.
**B. Speech-to-Text (STT)**
• **Model:** Whisper Tiny/Base, Moonshine Tiny.
• **Capabilities:** Real-time transcription, multi-language support, Voice Activity Detection (VAD).
• **API:** `transcribe(audioBuffer)`.
**C. Text-to-Speech (TTS)**
• **Model:** Kokoro-82M.
• **Capabilities:** Neural synthesis, adjustable pitch/rate, <100ms time-to-first-audio.
• **API:** `speak(text)`.

**5. Platform Matrix**

| **Platform** | **Integration Method** | **Acceleration** |
| --- | --- | --- |
| **Flutter (iOS/Android)** | **Dart FFI** | Metal / Vulkan |
| **Swift (iOS/macOS)** | Swift Package (XCFramework) | Metal |
| **Kotlin (Android)** | JNI / AAR Library | Vulkan / NNAPI |
| **React Native** | **JSI (TurboModules)** | Metal / Vulkan |
| **Flutter Web** | **WASM / Emscripten** | WebGPU |

**6. The 12-Week Roadmap
Month 1: The Foundation (Flutter & Core)**
• **W1 (C++ Core):** Setup CMake, compile `llama.cpp` for ARM64.
• **W2 (Flutter Mobile):** Build Dart FFI bindings and basic model loading logic.
• **W3 (Hardware Accel.):** Enable Metal and Vulkan for GPU-boosted inference.
• **W4 (Model Manager):** Implement local storage, checksums, and RAM-safe model loading.
**Month 2: Native Expansion**
• **W5 (Swift SDK):** Wrap C++ core into a native Swift Package for iOS devs.
• **W6 (Kotlin SDK):** Implement JNI layer for native Android/Kotlin support.
• **W7 (React Native):** Build JSI-based TurboModule for zero-latency JS calls.
• **W8 (Flutter Web):** Compile core to WASM and implement WebGPU fallback.
**Month 3: Ecosystem & Voice**
• **W9 (STT & VAD):** Integrate `whisper.cpp` and silence detection.
• **W10 (TTS & Voice Agent):** Launch the full "Voice Agent" pipeline (Audio-to-Audio).
• **W11 (Control Plane):** Build the OTA update system for remote model/prompt management.
• **W12 (Launch):** Documentation, "5-Minute Quick Starts," and marketing launch.

**7. Non-Functional Requirements**
• **Memory Safety:** The SDK must include a "Watchdog" to prevent the OS from killing the app when RAM exceeds 1.5GB on 4GB devices.
• **Binary Size:** Base SDK footprint (excluding models) should be <25MB.
• **Battery Impact:** Limit CPU-only inference to short bursts; favor GPU for long-form generation.

**8. Success Metrics (KPIs)**
1. **Latency:** Sub-500ms time-to-first-token on iPhone 13+ devices.
2. **Stability:** <0.1% crash rate across SDK instances.
3. **Developer Experience:** Time to "Hello World" chat under 10 minutes.