# Edge Veda SDK — Technical Audit

**Date:** 2026-02-09
**Version:** 1.1.0
**Scope:** Full codebase review — SDK, native layer, demo app, build system
**Codebase:** 121 commits, 18 Dart SDK files, 4 C/C++ source files, 7 demo app files

---

## Table of Contents

1. [Executive Summary](#1-executive-summary)
2. [Architecture Overview](#2-architecture-overview)
3. [Native C/C++ Core](#3-native-cc-core)
4. [Flutter SDK Layer](#4-flutter-sdk-layer)
5. [Chat Session API](#5-chat-session-api)
6. [Vision / Multimodal](#6-vision--multimodal)
7. [Production Runtime](#7-production-runtime)
8. [Build System & Packaging](#8-build-system--packaging)
9. [Demo Application](#9-demo-application)
10. [Performance Data](#10-performance-data)
11. [Security & Privacy](#11-security--privacy)
12. [Test Coverage](#12-test-coverage)
13. [Platform Status](#13-platform-status)
14. [Dependency Inventory](#14-dependency-inventory)
15. [File Inventory](#15-file-inventory)
16. [Known Limitations](#16-known-limitations)

---

## 1. Executive Summary

Edge Veda is a Flutter SDK for on-device LLM inference. It wraps [llama.cpp](https://github.com/ggml-org/llama.cpp) (version b7952) behind a Dart-native API that handles isolate safety, model lifecycle, streaming, memory monitoring, and build packaging.

**What it provides:**
- Text generation (blocking and streaming) via llama.cpp with Metal GPU on iOS
- Vision/multimodal inference (camera-to-text) via libmtmd from llama.cpp
- Multi-turn conversation management with context overflow summarization
- Adaptive runtime policy (thermal/battery/memory-aware QoS)
- Model download, caching, and checksum verification
- Pre-built XCFramework for iOS (device arm64 + simulator arm64)

**What it is not:**
- Not a custom inference engine — all tokens generated by unmodified llama.cpp
- Not cross-platform yet — iOS is fully working, Android is scaffolded but unvalidated
- No custom Metal/Vulkan kernels, no NPU/ANE integration, no custom quantization

**The core engineering contribution** is making llama.cpp work correctly inside Flutter's isolate model (where native pointers cannot cross isolate boundaries), combined with production-grade packaging (XCFramework, podspec symbol exports, persistent worker isolates).

---

## 2. Architecture Overview

```
┌─────────────────────────────────────────────────────┐
│                    Flutter App                       │
│  ┌──────────┐  ┌──────────┐  ┌──────────────────┐  │
│  │  Chat    │  │  Vision  │  │  Settings/Soak   │  │
│  │  Screen  │  │  Screen  │  │  Test Screen     │  │
│  └────┬─────┘  └────┬─────┘  └────────┬─────────┘  │
│       │              │                 │             │
│  ┌────▼──────────────▼─────────────────▼─────────┐  │
│  │              Edge Veda SDK (Dart)              │  │
│  │  ┌─────────────┐  ┌────────────┐              │  │
│  │  │ ChatSession │  │ ModelMgr   │              │  │
│  │  │ (multi-turn)│  │ (download) │              │  │
│  │  └──────┬──────┘  └────────────┘              │  │
│  │  ┌──────▼──────────────────────────────────┐  │  │
│  │  │         EdgeVeda (core class)           │  │  │
│  │  │  generate() / generateStream()          │  │  │
│  │  │  initVision() / describeImage()         │  │  │
│  │  └──────┬──────────────────┬───────────────┘  │  │
│  │  ┌──────▼──────┐  ┌───────▼──────────────┐   │  │
│  │  │ Streaming   │  │   VisionWorker       │   │  │
│  │  │ Worker      │  │   (persistent        │   │  │
│  │  │ (persistent │  │    vision isolate)    │   │  │
│  │  │  isolate)   │  │   + FrameQueue       │   │  │
│  │  └──────┬──────┘  └───────┬──────────────┘   │  │
│  │  ┌──────▼──────────────────▼──────────────┐   │  │
│  │  │     FFI Bindings (bindings.dart)        │   │  │
│  │  │     812 LOC, 37 native function ptrs    │   │  │
│  │  └──────┬──────────────────────────────────┘   │  │
│  └─────────┼──────────────────────────────────────┘  │
└────────────┼─────────────────────────────────────────┘
             │ DynamicLibrary.process() / dlsym
┌────────────▼─────────────────────────────────────────┐
│         XCFramework (libedge_veda_full.a)            │
│  ┌─────────┐ ┌─────────┐ ┌─────────┐ ┌──────────┐  │
│  │engine   │ │vision   │ │memory   │ │llama.cpp │  │
│  │.cpp     │ │_engine  │ │_guard   │ │(b7952)   │  │
│  │965 LOC  │ │.cpp     │ │.cpp     │ │+ Metal   │  │
│  │         │ │484 LOC  │ │625 LOC  │ │+ libmtmd │  │
│  └─────────┘ └─────────┘ └─────────┘ └──────────┘  │
└──────────────────────────────────────────────────────┘
```

### Key Design Decisions

| Decision | Rationale |
|----------|-----------|
| All FFI in `Isolate.run()` | Dart FFI is synchronous — would freeze UI. Native pointers never cross isolate boundary. |
| Persistent worker isolates | Text streaming and vision need long-lived contexts. `StreamingWorker` and `VisionWorker` keep models loaded. |
| Pull-based streaming | `ev_stream_next()` blocks until next token. Natural backpressure — Dart consumes as fast as C produces. |
| GGUF-only model format | Direct llama.cpp compatibility. No conversion step. |
| XCFramework with symbol whitelist | Podspec `-exported_symbol` flags ensure FFI visibility via `DynamicLibrary.process()`. |
| Pure Dart ChatSession | No new C symbols needed. Chat templates, conversation history, and summarization all in Dart. |

---

## 3. Native C/C++ Core

**4 files, 2,695 LOC total.** All C API functions are declared in `core/include/edge_veda.h` (621 lines).

### Source Files

| File | LOC | Purpose |
|------|-----|---------|
| `core/include/edge_veda.h` | 621 | Public C API header — 37 functions, structs, enums |
| `core/src/engine.cpp` | 965 | Text inference engine — wraps llama.cpp model/context lifecycle |
| `core/src/vision_engine.cpp` | 484 | Vision inference — wraps libmtmd for multimodal encoding |
| `core/src/memory_guard.cpp` | 625 | Cross-platform RSS monitoring with background thread |

### C API Surface (37 functions)

**Lifecycle:**
- `ev_init()` / `ev_free()` / `ev_is_valid()` — context creation/teardown
- `ev_config_default()` — default configuration
- `ev_version()` — version string

**Text Generation:**
- `ev_generate()` — blocking text generation
- `ev_generate_stream()` / `ev_stream_next()` / `ev_stream_has_next()` — streaming
- `ev_stream_cancel()` / `ev_stream_free()` — cancellation and cleanup
- `ev_generation_params_default()` — default sampling parameters
- `ev_free_string()` — free C-allocated output strings

**Vision:**
- `ev_vision_init()` / `ev_vision_free()` / `ev_vision_is_valid()` — VLM context
- `ev_vision_describe()` — image-to-text inference (RGB888 input)
- `ev_vision_config_default()` — default vision config
- `ev_vision_get_last_timings()` — timing data extraction

**Memory:**
- `ev_get_memory_usage()` / `ev_set_memory_limit()` — RSS tracking
- `ev_set_memory_pressure_callback()` — threshold-based callback
- `ev_memory_cleanup()` — KV cache clear + RSS refresh

**Utility:**
- `ev_detect_backend()` / `ev_is_backend_available()` / `ev_backend_name()` — GPU detection
- `ev_get_model_info()` — model metadata
- `ev_set_verbose()` — toggle llama.cpp logging
- `ev_get_last_error()` / `ev_error_string()` — error reporting
- `ev_reset()` — KV cache clear

### Memory Guard

Cross-platform RSS monitoring via platform-specific APIs:

| Platform | API | Metric |
|----------|-----|--------|
| iOS/macOS | `mach_task_info(MACH_TASK_BASIC_INFO)` | Resident set size |
| Android/Linux | `/proc/self/statm` | RSS in pages |
| Windows | `GetProcessMemoryInfo` | Working set size |

Features: background polling thread (1s interval, configurable 100ms–60s), callback at 90% threshold, atomic peak tracking with CAS.

---

## 4. Flutter SDK Layer

**18 files, 6,166 LOC total** (including FFI bindings, isolate workers, types, and utilities).

### Core Class: EdgeVeda (`edge_veda_impl.dart`, 896 LOC)

```dart
final edgeVeda = EdgeVeda();

// Initialize with model
await edgeVeda.init(EdgeVedaConfig(
  modelPath: '/path/to/model.gguf',
  numThreads: 4,
  contextLength: 2048,
  useGpu: true,
));

// Blocking generation
final response = await edgeVeda.generate('Hello', options: GenerateOptions(maxTokens: 256));
print(response.text);

// Streaming generation
await for (final chunk in edgeVeda.generateStream('Tell me a story')) {
  stdout.write(chunk.token);
}

// Vision
await edgeVeda.initVision(VisionConfig(modelPath: vlmPath, mmprojPath: mmprojPath));
final description = await edgeVeda.describeImage(rgbBytes, width: 640, height: 480);

// Cleanup
await edgeVeda.dispose();
```

### SDK File Breakdown

| File | LOC | Purpose |
|------|-----|---------|
| `edge_veda.dart` | 131 | Public barrel export (32 symbols) |
| `edge_veda_impl.dart` | 896 | Core EdgeVeda class |
| `types.dart` | 654 | Configs, responses, exceptions (10 typed exception classes) |
| `ffi/bindings.dart` | 812 | Native bindings — 37 function pointers via `DynamicLibrary.process()` |
| `ffi/native_memory.dart` | 550 | RAII scoping helpers for native allocations |
| `model_manager.dart` | 527 | Download, cache, verify, delete models |
| `isolate/worker_isolate.dart` | 448 | StreamingWorker — persistent text inference isolate |
| `isolate/vision_worker.dart` | 438 | VisionWorker — persistent vision inference isolate |
| `chat_session.dart` | 360 | Multi-turn conversation management |
| `runtime_policy.dart` | 258 | QoS adaptation with hysteresis |
| `chat_template.dart` | 214 | Llama 3 Instruct, ChatML, generic formatters |
| `telemetry_service.dart` | 178 | iOS thermal/battery/memory polling |
| `isolate/worker_messages.dart` | 156 | Sealed command/response types for text worker |
| `isolate/vision_worker_messages.dart` | 154 | Sealed command/response types for vision worker |
| `camera_utils.dart` | 146 | BGRA→RGB (iOS) and YUV→RGB (Android) converters |
| `frame_queue.dart` | 105 | Drop-newest backpressure queue for camera frames |
| `chat_types.dart` | 70 | ChatMessage, ChatRole, SystemPromptPreset |
| `perf_trace.dart` | 69 | JSONL performance trace logger |

### Exception Hierarchy

```
EdgeVedaException (abstract)
├── EdgeVedaGenericException
├── InitializationException
├── ModelLoadException
├── GenerationException
├── DownloadException
├── ChecksumException
├── ModelValidationException
├── MemoryException
├── ConfigurationException
└── VisionException
```

11 native error codes (`ev_error_t`) mapped to typed Dart exceptions via `NativeErrorCode.toException()`.

### Configuration Validation

Eager validation on main isolate before any FFI call:

| Parameter | Valid Range |
|-----------|------------|
| `numThreads` | 1–32 |
| `contextLength` | 128–32768 |
| `maxMemoryMb` | >= 256 |
| `temperature` | 0.0–2.0 |
| `topP` | 0.0–1.0 |
| `topK` | 1–100 |
| `repeatPenalty` | 0.0–2.0 |
| `maxTokens` | 1–32768 |

---

## 5. Chat Session API

**Pure Dart layer** (no new C symbols). Manages multi-turn conversation on top of `EdgeVeda`.

### API Surface

```dart
final session = ChatSession(
  edgeVeda: edgeVeda,
  preset: SystemPromptPreset.coder,       // or .assistant, .creative
  templateFormat: ChatTemplateFormat.llama3Instruct,  // default
);

// Blocking
final reply = await session.send('What is Dart?');

// Streaming
await for (final chunk in session.sendStream('Tell me more')) {
  stdout.write(chunk.token);
}

// State
session.messages;      // List<ChatMessage> (read-only)
session.turnCount;     // int (user turns)
session.contextUsage;  // double (0.0–1.0+, estimated)
session.isSummarizing; // bool

// Reset (model stays loaded)
session.reset();
```

### Chat Template Formats

| Format | Target Models |
|--------|---------------|
| `llama3Instruct` (default) | Llama 3.x Instruct family |
| `chatML` | ChatML-compatible models (Phi, Mistral, etc.) |
| `generic` | Fallback for other GGUF models |

### Context Overflow Handling

1. Token usage estimated at ~4 chars/token
2. Summarization triggers at **70%** of available context capacity
3. Keeps last 2 user turns + assistant replies intact
4. Older messages summarized by the model (temperature 0.3, max 128 tokens)
5. Summary stored as `ChatRole.summary` message
6. Fallback: simple truncation if summarization fails (drops oldest messages to 60%)

### System Prompt Presets

| Preset | Prompt |
|--------|--------|
| `assistant` | "You are a helpful assistant. Provide clear, concise answers." |
| `coder` | "You are a coding assistant. Provide clear, working code examples with brief explanations." |
| `creative` | "You are a creative writing assistant. Be imaginative, vivid, and engaging." |

System prompts are **immutable** after session creation — prevents mid-conversation behavioral shifts.

---

## 6. Vision / Multimodal

### Architecture

Vision inference uses `libmtmd` from llama.cpp's `tools/mtmd/` directory for multimodal encoding.

**Pipeline:**
1. Camera delivers BGRA8888 (iOS) or YUV420 (Android) frames
2. `CameraUtils` converts to RGB888 in pure Dart
3. RGB bytes sent to `VisionWorker` isolate via `SendPort`
4. Native: `mtmd_bitmap_init()` → `mtmd_tokenize()` → `mtmd_helper_eval_chunks()` → token generation
5. Description text + timing data returned to main isolate

### VisionWorker (Persistent Isolate)

Unlike one-shot `Isolate.run()`, the VisionWorker maintains the native vision context (~600MB) for the isolate's lifetime. The model loads once on `initVision()` and is reused for every `describeFrame()` call.

**Lifecycle:** `spawn()` → `initVision()` → `describeFrame()` (repeated) → `dispose()`

### FrameQueue (Backpressure)

Drop-newest policy with capacity 1:
- Camera callback calls `enqueue()` — if a frame is already pending, the old frame is replaced
- Processing loop calls `dequeue()` — returns null if already busy
- `droppedFrames` counter for telemetry/tracing

### Supported Vision Model

| Model | Size | Quantization |
|-------|------|-------------|
| SmolVLM2-500M-Video-Instruct | ~417MB | Q8_0 |
| SmolVLM2 mmproj | ~190MB | F16 |

### Timing Data

`ev_vision_get_last_timings()` returns per-frame breakdown:
- `modelLoadMs` — model loading time (0 after first frame with VisionWorker)
- `imageEncodeMs` — visual token encoding
- `promptEvalMs` — prompt evaluation
- `decodeMs` — token generation
- `promptTokens` / `generatedTokens` — counts

---

## 7. Production Runtime

### Telemetry Service

iOS platform telemetry via `MethodChannel`:
- `getThermalState()` — 0=nominal, 1=fair, 2=serious, 3=critical
- `getBatteryLevel()` — 0.0–1.0
- `getBatteryState()` — unknown/unplugged/charging/full
- `getMemoryRSS()` — process RSS in bytes
- `getAvailableMemory()` — `os_proc_available_memory()` in bytes
- `isLowPowerMode()` — Low Power Mode status
- `snapshot()` — all values concurrently via `Future.wait()`
- `thermalStateChanges` — push stream via `EventChannel`

Gracefully returns defaults on non-iOS platforms (catches `MissingPluginException`).

### Runtime Policy (QoS with Hysteresis)

Inspired by the TAPAS paper (arxiv 2501.02600). Adapts vision inference parameters based on device pressure signals.

**QoS Levels:**

| Level | Max FPS | Resolution | Max Tokens | Trigger |
|-------|---------|------------|------------|---------|
| `full` | 2 | 640px | 100 | No pressure |
| `reduced` | 1 | 480px | 75 | thermal>=1, avail<200MB, battery<15%, low power mode |
| `minimal` | 1 | 320px | 50 | thermal>=2, avail<100MB, battery<5% |
| `paused` | 0 | 0 | 0 | thermal>=3, avail<50MB |

**Hysteresis rules:**
- **Escalation** (degradation) is **immediate** — thermal spikes are dangerous
- **Restoration** (improvement) requires **cooldown** (60s default) and happens **one level at a time**
- Full restoration from paused→full requires 3 × cooldown (180s)

### PerfTrace (JSONL Logger)

Writes structured performance data for offline analysis:
```json
{"frame_id":0,"ts_ms":1738800000000,"stage":"image_encode","value":142.5}
{"frame_id":0,"ts_ms":1738800000200,"stage":"decode","value":830.2,"generated_tokens":47}
```

Analysis script: `analyze_trace.py` (Python, numpy + matplotlib) for statistical analysis of JSONL traces.

---

## 8. Build System & Packaging

### Build Files

| File | LOC | Purpose |
|------|-----|---------|
| `scripts/build-ios.sh` | 383 | iOS XCFramework build pipeline |
| `core/CMakeLists.txt` | 312 | CMake for C++ core + llama.cpp |
| `Makefile` | 380 | Top-level build orchestration |
| `flutter/ios/edge_veda.podspec` | 162 | CocoaPods integration |
| `flutter/android/build.gradle` | 142 | Android NDK/CMake integration |

### XCFramework Build Pipeline

`build-ios.sh --clean --release` produces:

1. Builds llama.cpp + edge_veda C code via CMake for:
   - `ios-arm64` (device)
   - `ios-arm64` (simulator, with Metal stubs)
2. Merges 7 static libraries into `libedge_veda_full.a`:
   - `libedge_veda.a`, `libllama.a`, `libggml.a`, `libggml-base.a`, `libggml-metal.a`, `libggml-cpu.a`, `libmtmd.a`
3. Creates XCFramework via `xcodebuild -create-xcframework`
4. Output: `flutter/ios/Frameworks/EdgeVedaCore.xcframework/` (~15MB)

### Podspec Symbol Export

The podspec uses `-force_load` to include the static library and 30+ `-exported_symbol` linker flags to ensure FFI symbols are visible via `DynamicLibrary.process()`:

```ruby
s.pod_target_xcconfig = {
  'OTHER_LDFLAGS[sdk=iphoneos*]' => [
    '-force_load', xcf_device_lib,
    '-Wl,-exported_symbol,_ev_init',
    '-Wl,-exported_symbol,_ev_free',
    '-Wl,-exported_symbol,_ev_generate',
    # ... 30+ symbols
    '-Wl,-exported_symbol,_main',  # Required for Xcode 26
  ]
}
```

**Xcode 26 compatibility note:** Must export `_main` and `___debug_main_executable_dylib_entry_point` because Xcode 26 compiles the app into `Runner.debug.dylib` and uses `dlsym()` for the entry point.

### llama.cpp Integration

- Version: **b7952** (upgraded from b4658 in Phase 8)
- Included as git submodule at `core/third_party/llama.cpp/`
- Unmodified source — no custom patches
- Metal GPU backend enabled for iOS
- Simulator builds require Metal framework stubs (b7952 unconditionally references Metal)

---

## 9. Demo Application

**7 files, 3,930 LOC total.** App name: "Veda" with Netflix-style bold red V logo.

### Screens

| Screen | File | LOC | Purpose |
|--------|------|-----|---------|
| Welcome | `welcome_screen.dart` | 156 | Branded onboarding with V logo animation |
| Chat | `main.dart` | 1,364 | Multi-turn chat with ChatSession, context indicator, persona picker |
| Vision | `vision_screen.dart` | 439 | Google Lens-style continuous camera scanning |
| Settings | `settings_screen.dart` | 777 | Temperature, tokens, storage, models, device info, about |
| Model Selection | `model_selection_modal.dart` | 270 | Bottom sheet model browser with device info |
| Soak Test | `soak_test_screen.dart` | 740 | 15-minute automated vision benchmark |
| Theme | `app_theme.dart` | 184 | Centralized dark theme with 17 color constants |

### Chat Tab Features

- **ChatSession integration:** All conversation via `_session.sendStream()` (no direct `generate()` calls)
- **Context indicator:** Shows turn count + usage percentage bar (accent color, warning at >80%)
- **New Chat button:** Resets session, clears messages, shows persona picker
- **Persona picker:** ChoiceChip row (Assistant, Coder, Creative) — shown only on fresh sessions
- **Streaming:** Progressive token display with TTFT and tokens/sec metrics
- **Summarization indicator:** Snackbar when context overflow triggers auto-summarization
- **Benchmark mode:** 10-prompt automated benchmark with targets (>15 tok/s, <1.2GB memory)

### Vision Tab Features

- Persistent VisionWorker isolate (model loads once)
- FrameQueue with drop-newest backpressure
- Real-time timing overlay (encode/eval/decode breakdown)
- Camera lifecycle management (pause on background, resume on foreground)

### Settings Tab Features

- Temperature and max tokens sliders
- Storage usage display with model deletion
- Device info (model name, memory, OS version via `sysctlbyname`)
- Model management section

### Soak Test Screen

- 15-minute automated vision inference benchmark
- Writes JSONL trace via PerfTrace
- Displays real-time stats: frames processed, dropped, p50 latency
- On-device soak test results (from validation):
  - Duration: 12.6 minutes
  - Frames processed: 254
  - p50 latency: 1,412ms
  - Crashes: 0
  - Model reloads: 0

### Model Registry

Pre-configured models available for download:

| Model | Size | Quantization |
|-------|------|-------------|
| Llama 3.2 1B Instruct | ~668MB | Q4_K_M |
| Phi 3.5 Mini Instruct | ~2.3GB | Q4_K_M |
| Gemma 2 2B Instruct | ~1.6GB | Q4_K_M |
| TinyLlama 1.1B Chat | ~669MB | Q4_K_M |
| SmolVLM2 500M (vision) | ~417MB | Q8_0 |
| SmolVLM2 mmproj | ~190MB | F16 |

Download features: progress tracking, SHA-256 verification, atomic temp-file-then-rename, retry with exponential backoff (3 attempts), `CancelToken` support.

---

## 10. Performance Data

### Soak Test Results (iPhone, Physical Device)

Validated on-device during Phase 11:

| Metric | Value |
|--------|-------|
| Duration | 12.6 minutes |
| Total frames | 254 |
| Effective FPS | ~0.34 (vision inference-limited) |
| p50 latency | 1,412ms |
| Dropped frames | N/A (backpressure via FrameQueue) |
| Crashes | 0 |
| Model reloads | 0 |
| Memory stability | Stable (no growth over session) |

### Text Inference

No systematic benchmark data published. Expected performance (from llama.cpp baselines):
- Q4_K_M 1B model on iPhone 15 Pro with Metal: ~30-60 tok/sec
- Demo app benchmark target: >15 tok/sec, <1.2GB peak memory

### Latency Breakdown (Vision, Per Frame)

Timing data extracted via `ev_vision_get_last_timings()`:
- `imageEncodeMs` — visual token encoding via libmtmd
- `promptEvalMs` — prompt processing
- `decodeMs` — token generation
- Total: typically 1.4s per frame (p50) on device

---

## 11. Security & Privacy

### Offline-First

All inference runs on-device via llama.cpp. Network calls occur only for optional model downloads (HTTP to HuggingFace URLs).

### Telemetry

**None.** Zero analytics, zero phone-home, zero crash reporting, zero usage tracking. No network calls during inference.

### Model Storage

- Plain GGUF files in `getApplicationSupportDirectory()` (survives cache clears)
- Not encrypted at rest
- SHA-256 checksum at download time only (no runtime integrity checks)
- No jailbreak/root detection

### Logging

- `verbose=false` (default): only errors logged
- `verbose=true`: llama.cpp logs to stderr (may include prompt content in debug builds)
- No log scrubbing or PII detection

---

## 12. Test Coverage

| File | LOC | Tests |
|------|-----|-------|
| `flutter/test/edge_veda_test.dart` | 253 | 14 unit tests (config validation, error mapping, types) |
| `core/tests/test_inference.cpp` | 237 | C++ smoke test (model load, generate, stream) |
| `flutter/example/test/widget_test.dart` | 22 | Widget smoke test |
| **Total** | **512** | |

No integration tests. No end-to-end tests. No automated UI tests. The soak test screen provides manual stress testing for vision inference.

---

## 13. Platform Status

| Platform | Status | GPU | Notes |
|----------|--------|-----|-------|
| **iOS (device)** | Working | Metal | Validated on iPhone with iOS 26.2.1 |
| **iOS (simulator)** | Working | CPU only | Metal stubs for compilation |
| **Android** | Scaffolded | CPU only | Plugin exists, CMake wired, APK build unvalidated |
| **Android (Vulkan)** | Not started | — | Deferred to Phase 7 |
| **macOS/Windows/Linux** | Not supported | — | — |
| **Web** | Not supported | — | — |

### iOS Environment

- Flutter 3.38.9, Dart 3.10.8
- Xcode 26.1 (17B55), macOS 26.2
- XCFramework: device arm64 + simulator arm64, ~15MB

### Android Status

- `EdgeVedaPlugin.kt` handles memory pressure forwarding via `EventChannel`
- `build.gradle` wires CMake for NDK compilation
- NDK r27c pinned (r28+ has 16KB page alignment issues with mmap)
- Phase 5 (APK build), Phase 6 (integration tests), Phase 7 (Vulkan) all pending

---

## 14. Dependency Inventory

### Dart Dependencies (SDK)

| Package | Version | Purpose |
|---------|---------|---------|
| `ffi` | >=2.0.0 <2.1.0 | C interop types (Pointer, Struct, calloc) |
| `path_provider` | ^2.1.0 | Platform storage directories |
| `path` | ^1.8.0 | Path manipulation |
| `crypto` | ^3.0.0 | SHA-256 model checksums |
| `http` | ^1.0.0 | Model downloads |

**Note:** ffi is constrained to <2.1.0 because ffi 2.1.0+ pulls in `objective_c` 9.2.5 which crashes on iOS simulator with "DOBJC_initializeApi" error.

### Native Dependencies

| Library | Version | Purpose |
|---------|---------|---------|
| llama.cpp | b7952 | LLM inference engine |
| libmtmd | (from llama.cpp) | Multimodal/vision encoding |
| Metal.framework | System | iOS GPU acceleration |
| Accelerate.framework | System | BLAS operations |

### Demo App Additional Dependencies

| Package | Version | Purpose |
|---------|---------|---------|
| `camera` | ^0.11.0 | Camera access for vision |
| `flutter` | SDK | Framework |

---

## 15. File Inventory

### C++ Core (4 files, 2,695 LOC)
```
core/include/edge_veda.h         621 LOC  Public C API
core/src/engine.cpp              965 LOC  Text inference engine
core/src/vision_engine.cpp       484 LOC  Vision/VLM engine
core/src/memory_guard.cpp        625 LOC  Memory monitoring
```

### Flutter SDK (18 files, 6,166 LOC)
```
flutter/lib/edge_veda.dart                           131 LOC  Barrel exports
flutter/lib/src/edge_veda_impl.dart                  896 LOC  Core EdgeVeda class
flutter/lib/src/types.dart                           654 LOC  Types, configs, exceptions
flutter/lib/src/ffi/bindings.dart                    812 LOC  FFI bindings
flutter/lib/src/ffi/native_memory.dart               550 LOC  RAII scoping
flutter/lib/src/model_manager.dart                   527 LOC  Download/cache
flutter/lib/src/isolate/worker_isolate.dart          448 LOC  Streaming worker
flutter/lib/src/isolate/vision_worker.dart           438 LOC  Vision worker
flutter/lib/src/chat_session.dart                    360 LOC  Chat session
flutter/lib/src/runtime_policy.dart                  258 LOC  QoS policy
flutter/lib/src/chat_template.dart                   214 LOC  Chat templates
flutter/lib/src/telemetry_service.dart               178 LOC  iOS telemetry
flutter/lib/src/isolate/worker_messages.dart         156 LOC  Text worker msgs
flutter/lib/src/isolate/vision_worker_messages.dart  154 LOC  Vision worker msgs
flutter/lib/src/camera_utils.dart                    146 LOC  Image conversion
flutter/lib/src/frame_queue.dart                     105 LOC  Backpressure queue
flutter/lib/src/chat_types.dart                       70 LOC  Chat types
flutter/lib/src/perf_trace.dart                       69 LOC  JSONL trace
```

### Demo App (7 files, 3,930 LOC)
```
flutter/example/lib/main.dart                       1364 LOC  Chat screen + home
flutter/example/lib/settings_screen.dart             777 LOC  Settings
flutter/example/lib/soak_test_screen.dart             740 LOC  Soak test
flutter/example/lib/vision_screen.dart                439 LOC  Vision camera
flutter/example/lib/model_selection_modal.dart        270 LOC  Model browser
flutter/example/lib/app_theme.dart                    184 LOC  Dark theme
flutter/example/lib/welcome_screen.dart               156 LOC  Onboarding
```

### Build System (5 files, 1,379 LOC)
```
scripts/build-ios.sh              383 LOC  XCFramework build
Makefile                          380 LOC  Top-level orchestration
core/CMakeLists.txt               312 LOC  CMake for C++ core
flutter/ios/edge_veda.podspec     162 LOC  CocoaPods config
flutter/android/build.gradle      142 LOC  Android NDK/CMake
```

### Tests (3 files, 512 LOC)
```
flutter/test/edge_veda_test.dart         253 LOC  Dart unit tests
core/tests/test_inference.cpp            237 LOC  C++ smoke test
flutter/example/test/widget_test.dart     22 LOC  Widget smoke test
```

### Total: 32 source files, ~14,700 LOC

---

## 16. Known Limitations

| Area | Limitation |
|------|-----------|
| **Inference engine** | Unmodified llama.cpp — no custom optimizations, speculative decoding, or KV cache tricks |
| **Android** | Scaffolded but unvalidated — no confirmed end-to-end device test |
| **Vulkan GPU** | Not implemented (deferred to Phase 7) |
| **NPU/ANE** | Not supported (no CoreML, NNAPI, or Apple Neural Engine integration) |
| **Token counting** | Estimated at ~4 chars/token — exact tokenization not exposed |
| **Stop sequences** | Declared in API but not wired to native sampling |
| **Image copies** | 3–4 copies per camera frame (no zero-copy path) |
| **Model format** | GGUF only — no ONNX, CoreML, TFLite, or ExecuTorch |
| **Background execution** | No iOS background modes or Android foreground service |
| **Thermal adaptation** | RuntimePolicy exists but TelemetryService native implementation only partially wired (simulator-safe defaults) |
| **Content filtering** | Not implemented |
| **Model encryption** | Not implemented — plain GGUF on disk |

---

*Generated 2026-02-09. Codebase at commit `0899d20` (121 commits on main).*
